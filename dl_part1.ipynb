{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/paridhimaheshwari/workspace/ml-projects/nlp/train_cleaned.csv\")\n",
    "df1 = pd.read_csv(\"/Users/paridhimaheshwari/workspace/ml-projects/nlp/test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      textID                                               text  \\\n",
       "0           1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "1           2  088c60f138                          my boss is bullying me...   \n",
       "2           3  9642c003ef                     what interview! leave me alone   \n",
       "3           4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "4           6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "\n",
       "    selected_text sentiment  \n",
       "0        Sooo SAD  negative  \n",
       "1     bullying me  negative  \n",
       "2  leave me alone  negative  \n",
       "3   Sons of ****,  negative  \n",
       "4             fun  positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:5000,'text']\n",
    "X_test=df1.loc[:5000,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(df.loc[:5000,'sentiment'])\n",
    "Y_test = le.transform(df1.loc[:5000,'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "X_train1 = tf.fit_transform(X_train)\n",
    "X_test1 = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = pd.DataFrame(X_train1.toarray())\n",
    "X_test1 = pd.DataFrame(X_test1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8671</th>\n",
       "      <th>8672</th>\n",
       "      <th>8673</th>\n",
       "      <th>8674</th>\n",
       "      <th>8675</th>\n",
       "      <th>8676</th>\n",
       "      <th>8677</th>\n",
       "      <th>8678</th>\n",
       "      <th>8679</th>\n",
       "      <th>8680</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows Ã— 8681 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  8671  \\\n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "4996   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4997   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4998   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4999   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "5000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "      8672  8673  8674  8675  8676  8677  8678  8679  8680  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "4996   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4997   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4998   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4999   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5001 rows x 8681 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training phase\n",
    "def training_phase(X, Y, lr, iterations):\n",
    "    n=len(X)\n",
    "    X = X.T\n",
    "    Y = Y.T     #taken transpose for the purpose of deep learning\n",
    "    W=np.random.randint(-10,10,size=(X.shape[0],1))*0.01\n",
    "    b=0\n",
    "    for i in range(iterations):\n",
    "        # forward pass\n",
    "        A = sigmoid(np.dot(W.T,X)+b)      #this is activation\n",
    "        cost = (-1/n)*np.sum((Y*np.log(A))+((1-Y)*np.log(1-A)))\n",
    "        print(\"iteration: \"+str(i)+', cost: '+str(cost))\n",
    "        # backward pass\n",
    "        dW = (1/n)*np.dot(X, (A-Y).T)\n",
    "        db = (1/n)*np.sum((A-Y).T)\n",
    "        W = W-lr*dW\n",
    "        b = b-lr*db\n",
    "    return W, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, cost: 0.6937490020366454\n",
      "iteration: 1, cost: 0.6921749485567315\n",
      "iteration: 2, cost: 0.6945202073874504\n",
      "iteration: 3, cost: 0.705773551876703\n",
      "iteration: 4, cost: 0.7311093875651006\n",
      "iteration: 5, cost: 0.7862680134941753\n",
      "iteration: 6, cost: 0.8500804196769881\n",
      "iteration: 7, cost: 0.9476065149813773\n",
      "iteration: 8, cost: 0.9481536092785049\n",
      "iteration: 9, cost: 1.0179506763970134\n",
      "iteration: 10, cost: 0.9517621211641561\n",
      "iteration: 11, cost: 1.01433533058399\n",
      "iteration: 12, cost: 0.9425538014634779\n",
      "iteration: 13, cost: 1.003677141865378\n",
      "iteration: 14, cost: 0.9330196612307673\n",
      "iteration: 15, cost: 0.9926718993616558\n",
      "iteration: 16, cost: 0.9235305581073358\n",
      "iteration: 17, cost: 0.9815379435393515\n",
      "iteration: 18, cost: 0.9140987173687253\n",
      "iteration: 19, cost: 0.9703094944124581\n",
      "iteration: 20, cost: 0.904725139696891\n",
      "iteration: 21, cost: 0.9590144829038267\n",
      "iteration: 22, cost: 0.8954093520796008\n",
      "iteration: 23, cost: 0.9476789052380997\n",
      "iteration: 24, cost: 0.8861500959262969\n",
      "iteration: 25, cost: 0.9363266472520246\n",
      "iteration: 26, cost: 0.8769456683307796\n",
      "iteration: 27, cost: 0.9249792942363694\n",
      "iteration: 28, cost: 0.8677941824103431\n",
      "iteration: 29, cost: 0.9136560557430442\n",
      "iteration: 30, cost: 0.8586937620536986\n",
      "iteration: 31, cost: 0.9023737915681376\n",
      "iteration: 32, cost: 0.8496426802105486\n",
      "iteration: 33, cost: 0.8911471154356874\n",
      "iteration: 34, cost: 0.8406394505360368\n",
      "iteration: 35, cost: 0.879988552107033\n",
      "iteration: 36, cost: 0.8316828820214374\n",
      "iteration: 37, cost: 0.8689087260467747\n",
      "iteration: 38, cost: 0.822772105333831\n",
      "iteration: 39, cost: 0.8579165638052008\n",
      "iteration: 40, cost: 0.8139065783252379\n",
      "iteration: 41, cost: 0.8470194967830738\n",
      "iteration: 42, cost: 0.8050860768290388\n",
      "iteration: 43, cost: 0.8362236552694011\n",
      "iteration: 44, cost: 0.796310675601386\n",
      "iteration: 45, cost: 0.8255340481830774\n",
      "iteration: 46, cost: 0.7875807231646221\n",
      "iteration: 47, cost: 0.8149547256702145\n",
      "iteration: 48, cost: 0.7788968133900931\n",
      "iteration: 49, cost: 0.8044889236455097\n",
      "iteration: 50, cost: 0.7702597559107471\n",
      "iteration: 51, cost: 0.7941391906399426\n",
      "iteration: 52, cost: 0.7616705468584608\n",
      "iteration: 53, cost: 0.7839074980784382\n",
      "iteration: 54, cost: 0.7531303409532847\n",
      "iteration: 55, cost: 0.7737953355039798\n",
      "iteration: 56, cost: 0.7446404256096525\n",
      "iteration: 57, cost: 0.7638037924099536\n",
      "iteration: 58, cost: 0.7362021974491191\n",
      "iteration: 59, cost: 0.7539336283334892\n",
      "iteration: 60, cost: 0.7278171414046459\n",
      "iteration: 61, cost: 0.7441853327661484\n",
      "iteration: 62, cost: 0.7194868124545502\n",
      "iteration: 63, cost: 0.7345591762995195\n",
      "iteration: 64, cost: 0.7112128199237758\n",
      "iteration: 65, cost: 0.7250552542705281\n",
      "iteration: 66, cost: 0.7029968142264135\n",
      "iteration: 67, cost: 0.7156735240211354\n",
      "iteration: 68, cost: 0.694840475888068\n",
      "iteration: 69, cost: 0.706413836748204\n",
      "iteration: 70, cost: 0.6867455066726256\n",
      "iteration: 71, cost: 0.6972759647953044\n",
      "iteration: 72, cost: 0.6787136226392109\n",
      "iteration: 73, cost: 0.6882596251298176\n",
      "iteration: 74, cost: 0.6707465489668243\n",
      "iteration: 75, cost: 0.6793644996550965\n",
      "iteration: 76, cost: 0.6628460164024087\n",
      "iteration: 77, cost: 0.6705902529271232\n",
      "iteration: 78, cost: 0.6550137592100282\n",
      "iteration: 79, cost: 0.6619365477764092\n",
      "iteration: 80, cost: 0.6472515145222713\n",
      "iteration: 81, cost: 0.653403059277176\n",
      "iteration: 82, cost: 0.6395610230184569\n",
      "iteration: 83, cost: 0.6449894874557295\n",
      "iteration: 84, cost: 0.6319440308767345\n",
      "iteration: 85, cost: 0.6366955690871148\n",
      "iteration: 86, cost: 0.6244022929681337\n",
      "iteration: 87, cost: 0.6285210888925782\n",
      "iteration: 88, cost: 0.6169375772797089\n",
      "iteration: 89, cost: 0.6204658904191058\n",
      "iteration: 90, cost: 0.6095516705709159\n",
      "iteration: 91, cost: 0.612529886855462\n",
      "iteration: 92, cost: 0.60224638528213\n",
      "iteration: 93, cost: 0.6047130720158714\n",
      "iteration: 94, cost: 0.5950235677265568\n",
      "iteration: 95, cost: 0.5970155317018221\n",
      "iteration: 96, cost: 0.5878851076064064\n",
      "iteration: 97, cost: 0.5894374556333885\n",
      "iteration: 98, cost: 0.5808329489005927\n",
      "iteration: 99, cost: 0.5819791501227668\n",
      "iteration: 100, cost: 0.5738691021736221\n",
      "iteration: 101, cost: 0.5746410516429009\n",
      "iteration: 102, cost: 0.5669956583526181\n",
      "iteration: 103, cost: 0.5674237414213625\n",
      "iteration: 104, cost: 0.5602148040100526\n",
      "iteration: 105, cost: 0.5603279611617792\n",
      "iteration: 106, cost: 0.5535288381716036\n",
      "iteration: 107, cost: 0.5533546299593434\n",
      "iteration: 108, cost: 0.5469401906389167\n",
      "iteration: 109, cost: 0.5465048624298526\n",
      "iteration: 110, cost: 0.5404514417723926\n",
      "iteration: 111, cost: 0.5397799880091728\n",
      "iteration: 112, cost: 0.5340653436151249\n",
      "iteration: 113, cost: 0.5331815712969968\n",
      "iteration: 114, cost: 0.5277848421504855\n",
      "iteration: 115, cost: 0.5267114332093796\n",
      "iteration: 116, cost: 0.5216131003663661\n",
      "iteration: 117, cost: 0.5203716725620445\n",
      "iteration: 118, cost: 0.5155535216416992\n",
      "iteration: 119, cost: 0.5141646875234562\n",
      "iteration: 120, cost: 0.5096097727679544\n",
      "iteration: 121, cost: 0.5080931961455111\n",
      "iteration: 122, cost: 0.5037858056623042\n",
      "iteration: 123, cost: 0.5021602548932713\n",
      "iteration: 124, cost: 0.4980858765135129\n",
      "iteration: 125, cost: 0.49636927374821005\n",
      "iteration: 126, cost: 0.49251456072245836\n",
      "iteration: 127, cost: 0.4907240260503944\n",
      "iteration: 128, cost: 0.487076761557738\n",
      "iteration: 129, cost: 0.48522865077907285\n",
      "iteration: 130, cost: 0.48177770995255437\n",
      "iteration: 131, cost: 0.47988764446380183\n",
      "iteration: 132, cost: 0.4766229523444273\n",
      "iteration: 133, cost: 0.4747058394004205\n",
      "iteration: 134, cost: 0.4716183229446866\n",
      "iteration: 135, cost: 0.4696883643696892\n",
      "iteration: 136, cost: 0.4667698963847119\n",
      "iteration: 137, cost: 0.4648405836996704\n",
      "iteration: 138, cost: 0.46208391641448343\n",
      "iteration: 139, cost: 0.46016801038549154\n",
      "iteration: 140, cost: 0.457566696352796\n",
      "iteration: 141, cost: 0.4556761892235323\n",
      "iteration: 142, cost: 0.4532244874649276\n",
      "iteration: 143, cost: 0.45137054669907967\n",
      "iteration: 144, cost: 0.44906331255042387\n",
      "iteration: 145, cost: 0.4472562058645815\n",
      "iteration: 146, cost: 0.44508876393569063\n",
      "iteration: 147, cost: 0.4433377668157274\n",
      "iteration: 148, cost: 0.44130576791519527\n",
      "iteration: 149, cost: 0.4396190567008434\n",
      "iteration: 150, cost: 0.4377183215034697\n",
      "iteration: 151, cost: 0.43610285743826493\n",
      "iteration: 152, cost: 0.4343292120111093\n",
      "iteration: 153, cost: 0.4327906242160012\n",
      "iteration: 154, cost: 0.43113973506847775\n",
      "iteration: 155, cost: 0.42968221289720293\n",
      "iteration: 156, cost: 0.42814943163899766\n",
      "iteration: 157, cost: 0.4267756388658332\n",
      "iteration: 158, cost: 0.42535586837435174\n",
      "iteration: 159, cost: 0.42406689261210795\n",
      "iteration: 160, cost: 0.4227544872915223\n",
      "iteration: 161, cost: 0.42154983740572977\n",
      "iteration: 162, cost: 0.42033854916944574\n",
      "iteration: 163, cost: 0.41921621085687466\n",
      "iteration: 164, cost: 0.4180991895854522\n",
      "iteration: 165, cost: 0.4170557446425653\n",
      "iteration: 166, cost: 0.41602559710597076\n",
      "iteration: 167, cost: 0.4150564055822454\n",
      "iteration: 168, cost: 0.4141053106558393\n",
      "iteration: 169, cost: 0.4132047478924993\n",
      "iteration: 170, cost: 0.41232461925221986\n",
      "iteration: 171, cost: 0.4114863529333145\n",
      "iteration: 172, cost: 0.4106690344668575\n",
      "iteration: 173, cost: 0.4098863215526666\n",
      "iteration: 174, cost: 0.40912379664222825\n",
      "iteration: 175, cost: 0.4083897774911944\n",
      "iteration: 176, cost: 0.40767437195796546\n",
      "iteration: 177, cost: 0.4069823396236736\n",
      "iteration: 178, cost: 0.406306899752021\n",
      "iteration: 179, cost: 0.4056505262222942\n",
      "iteration: 180, cost: 0.40500855752366827\n",
      "iteration: 181, cost: 0.4043820647467408\n",
      "iteration: 182, cost: 0.4037678230572783\n",
      "iteration: 183, cost: 0.40316609374302204\n",
      "iteration: 184, cost: 0.40257462667504135\n",
      "iteration: 185, cost: 0.4019932567438356\n",
      "iteration: 186, cost: 0.4014203992751499\n",
      "iteration: 187, cost: 0.40085569936465504\n",
      "iteration: 188, cost: 0.4002980316076795\n",
      "iteration: 189, cost: 0.39974698859810387\n",
      "iteration: 190, cost: 0.3992017661490536\n",
      "iteration: 191, cost: 0.39866197709353224\n",
      "iteration: 192, cost: 0.3981270448682106\n",
      "iteration: 193, cost: 0.39759663529827943\n",
      "iteration: 194, cost: 0.3970703347879353\n",
      "iteration: 195, cost: 0.3965478716392974\n",
      "iteration: 196, cost: 0.3960289496019296\n",
      "iteration: 197, cost: 0.395513356581397\n",
      "iteration: 198, cost: 0.39500088086258694\n",
      "iteration: 199, cost: 0.39449136149504327\n",
      "iteration: 200, cost: 0.39398464739056105\n",
      "iteration: 201, cost: 0.39348061863981604\n",
      "iteration: 202, cost: 0.39297916726350524\n",
      "iteration: 203, cost: 0.3924802047392086\n",
      "iteration: 204, cost: 0.39198365338724667\n",
      "iteration: 205, cost: 0.39148944780949824\n",
      "iteration: 206, cost: 0.39099753133793674\n",
      "iteration: 207, cost: 0.39050785510300917\n",
      "iteration: 208, cost: 0.3900203768014286\n",
      "iteration: 209, cost: 0.3895350590858043\n",
      "iteration: 210, cost: 0.38905186933784563\n",
      "iteration: 211, cost: 0.38857077808015517\n",
      "iteration: 212, cost: 0.38809175914459143\n",
      "iteration: 213, cost: 0.3876147883462325\n",
      "iteration: 214, cost: 0.3871398437740881\n",
      "iteration: 215, cost: 0.3866669047680211\n",
      "iteration: 216, cost: 0.38619595221674374\n",
      "iteration: 217, cost: 0.3857269678029948\n",
      "iteration: 218, cost: 0.385259934265519\n",
      "iteration: 219, cost: 0.3847948348574236\n",
      "iteration: 220, cost: 0.38433165356039495\n",
      "iteration: 221, cost: 0.38387037470305924\n",
      "iteration: 222, cost: 0.38341098312881744\n",
      "iteration: 223, cost: 0.3829534639302605\n",
      "iteration: 224, cost: 0.3824978025768329\n",
      "iteration: 225, cost: 0.38204398473158824\n",
      "iteration: 226, cost: 0.3815919963460669\n",
      "iteration: 227, cost: 0.38114182353460446\n",
      "iteration: 228, cost: 0.3806934526434394\n",
      "iteration: 229, cost: 0.38024687016482606\n",
      "iteration: 230, cost: 0.3798020627864353\n",
      "iteration: 231, cost: 0.3793590173327854\n",
      "iteration: 232, cost: 0.378917720799903\n",
      "iteration: 233, cost: 0.3784781603153906\n",
      "iteration: 234, cost: 0.37804032316227265\n",
      "iteration: 235, cost: 0.3776041967517169\n",
      "iteration: 236, cost: 0.377169768639079\n",
      "iteration: 237, cost: 0.3767370265051833\n",
      "iteration: 238, cost: 0.37630595816682744\n",
      "iteration: 239, cost: 0.375876551563833\n",
      "iteration: 240, cost: 0.3754487947656741\n",
      "iteration: 241, cost: 0.37502267596240635\n",
      "iteration: 242, cost: 0.37459818346862295\n",
      "iteration: 243, cost: 0.3741753057169829\n",
      "iteration: 244, cost: 0.37375403126035106\n",
      "iteration: 245, cost: 0.3733343487670663\n",
      "iteration: 246, cost: 0.37291624702186443\n",
      "iteration: 247, cost: 0.3724997149223099\n",
      "iteration: 248, cost: 0.37208474147891785\n",
      "iteration: 249, cost: 0.3716713158123638\n",
      "iteration: 250, cost: 0.37125942715308785\n",
      "iteration: 251, cost: 0.37084906483902735\n",
      "iteration: 252, cost: 0.37044021831489415\n",
      "iteration: 253, cost: 0.370032877130261\n",
      "iteration: 254, cost: 0.36962703093863974\n",
      "iteration: 255, cost: 0.3692226694958093\n",
      "iteration: 256, cost: 0.368819782658779\n",
      "iteration: 257, cost: 0.36841836038428544\n",
      "iteration: 258, cost: 0.3680183927276951\n",
      "iteration: 259, cost: 0.36761986984162204\n",
      "iteration: 260, cost: 0.36722278197480573\n",
      "iteration: 261, cost: 0.36682711947081753\n",
      "iteration: 262, cost: 0.36643287276693587\n",
      "iteration: 263, cost: 0.36604003239292165\n",
      "iteration: 264, cost: 0.36564858896990404\n",
      "iteration: 265, cost: 0.36525853320921153\n",
      "iteration: 266, cost: 0.36486985591127674\n",
      "iteration: 267, cost: 0.364482547964514\n",
      "iteration: 268, cost: 0.36409660034424807\n",
      "iteration: 269, cost: 0.36371200411163224\n",
      "iteration: 270, cost: 0.36332875041260365\n",
      "iteration: 271, cost: 0.36294683047683773\n",
      "iteration: 272, cost: 0.3625662356167312\n",
      "iteration: 273, cost: 0.36218695722639\n",
      "iteration: 274, cost: 0.3618089867806404\n",
      "iteration: 275, cost: 0.361432315834048\n",
      "iteration: 276, cost: 0.3610569360199571\n",
      "iteration: 277, cost: 0.3606828390495391\n",
      "iteration: 278, cost: 0.36031001671085877\n",
      "iteration: 279, cost: 0.3599384608679518\n",
      "iteration: 280, cost: 0.35956816345991716\n",
      "iteration: 281, cost: 0.3591991165000217\n",
      "iteration: 282, cost: 0.3588313120748188\n",
      "iteration: 283, cost: 0.3584647423432788\n",
      "iteration: 284, cost: 0.3580993995359326\n",
      "iteration: 285, cost: 0.35773527595402776\n",
      "iteration: 286, cost: 0.35737236396869587\n",
      "iteration: 287, cost: 0.3570106560201328\n",
      "iteration: 288, cost: 0.3566501446167906\n",
      "iteration: 289, cost: 0.35629082233457987\n",
      "iteration: 290, cost: 0.3559326818160852\n",
      "iteration: 291, cost: 0.35557571576978997\n",
      "iteration: 292, cost: 0.3552199169693137\n",
      "iteration: 293, cost: 0.3548652782526592\n",
      "iteration: 294, cost: 0.35451179252147036\n",
      "iteration: 295, cost: 0.35415945274030103\n",
      "iteration: 296, cost: 0.3538082519358934\n",
      "iteration: 297, cost: 0.3534581831964672\n",
      "iteration: 298, cost: 0.353109239671018\n",
      "iteration: 299, cost: 0.3527614145686261\n",
      "iteration: 300, cost: 0.3524147011577742\n",
      "iteration: 301, cost: 0.3520690927656755\n",
      "iteration: 302, cost: 0.35172458277761\n",
      "iteration: 303, cost: 0.3513811646362709\n",
      "iteration: 304, cost: 0.3510388318411194\n",
      "iteration: 305, cost: 0.35069757794774825\n",
      "iteration: 306, cost: 0.3503573965672548\n",
      "iteration: 307, cost: 0.35001828136562163\n",
      "iteration: 308, cost: 0.3496802260631064\n",
      "iteration: 309, cost: 0.34934322443363924\n",
      "iteration: 310, cost: 0.3490072703042291\n",
      "iteration: 311, cost: 0.3486723575543777\n",
      "iteration: 312, cost: 0.348338480115501\n",
      "iteration: 313, cost: 0.34800563197035944\n",
      "iteration: 314, cost: 0.34767380715249474\n",
      "iteration: 315, cost: 0.3473429997456752\n",
      "iteration: 316, cost: 0.3470132038833474\n",
      "iteration: 317, cost: 0.3466844137480964\n",
      "iteration: 318, cost: 0.34635662357111197\n",
      "iteration: 319, cost: 0.3460298276316627\n",
      "iteration: 320, cost: 0.3457040202565765\n",
      "iteration: 321, cost: 0.34537919581972837\n",
      "iteration: 322, cost: 0.3450553487415348\n",
      "iteration: 323, cost: 0.3447324734884547\n",
      "iteration: 324, cost: 0.34441056457249686\n",
      "iteration: 325, cost: 0.3440896165507339\n",
      "iteration: 326, cost: 0.34376962402482286\n",
      "iteration: 327, cost: 0.3434505816405315\n",
      "iteration: 328, cost: 0.34313248408727076\n",
      "iteration: 329, cost: 0.34281532609763427\n",
      "iteration: 330, cost: 0.34249910244694204\n",
      "iteration: 331, cost: 0.34218380795279185\n",
      "iteration: 332, cost: 0.34186943747461523\n",
      "iteration: 333, cost: 0.3415559859132394\n",
      "iteration: 334, cost: 0.3412434482104551\n",
      "iteration: 335, cost: 0.34093181934858974\n",
      "iteration: 336, cost: 0.3406210943500859\n",
      "iteration: 337, cost: 0.3403112682770854\n",
      "iteration: 338, cost: 0.34000233623101844\n",
      "iteration: 339, cost: 0.33969429335219786\n",
      "iteration: 340, cost: 0.33938713481941896\n",
      "iteration: 341, cost: 0.33908085584956404\n",
      "iteration: 342, cost: 0.3387754516972119\n",
      "iteration: 343, cost: 0.338470917654252\n",
      "iteration: 344, cost: 0.3381672490495045\n",
      "iteration: 345, cost: 0.3378644412483433\n",
      "iteration: 346, cost: 0.3375624896523255\n",
      "iteration: 347, cost: 0.3372613896988242\n",
      "iteration: 348, cost: 0.3369611368606669\n",
      "iteration: 349, cost: 0.33666172664577737\n",
      "iteration: 350, cost: 0.3363631545968226\n",
      "iteration: 351, cost: 0.33606541629086367\n",
      "iteration: 352, cost: 0.3357685073390117\n",
      "iteration: 353, cost: 0.33547242338608674\n",
      "iteration: 354, cost: 0.3351771601102821\n",
      "iteration: 355, cost: 0.334882713222832\n",
      "iteration: 356, cost: 0.3345890784676833\n",
      "iteration: 357, cost: 0.33429625162117194\n",
      "iteration: 358, cost: 0.3340042284917021\n",
      "iteration: 359, cost: 0.3337130049194305\n",
      "iteration: 360, cost: 0.3334225767759536\n",
      "iteration: 361, cost: 0.33313293996399895\n",
      "iteration: 362, cost: 0.3328440904171202\n",
      "iteration: 363, cost: 0.3325560240993956\n",
      "iteration: 364, cost: 0.33226873700513077\n",
      "iteration: 365, cost: 0.33198222515856374\n",
      "iteration: 366, cost: 0.3316964846135749\n",
      "iteration: 367, cost: 0.3314115114533996\n",
      "iteration: 368, cost: 0.3311273017903441\n",
      "iteration: 369, cost: 0.33084385176550535\n",
      "iteration: 370, cost: 0.33056115754849386\n",
      "iteration: 371, cost: 0.33027921533715954\n",
      "iteration: 372, cost: 0.3299980213573214\n",
      "iteration: 373, cost: 0.3297175718624999\n",
      "iteration: 374, cost: 0.32943786313365236\n",
      "iteration: 375, cost: 0.32915889147891225\n",
      "iteration: 376, cost: 0.3288806532333303\n",
      "iteration: 377, cost: 0.3286031447586198\n",
      "iteration: 378, cost: 0.32832636244290414\n",
      "iteration: 379, cost: 0.3280503027004674\n",
      "iteration: 380, cost: 0.3277749619715082\n",
      "iteration: 381, cost: 0.32750033672189566\n",
      "iteration: 382, cost: 0.32722642344292907\n",
      "iteration: 383, cost: 0.3269532186510994\n",
      "iteration: 384, cost: 0.3266807188878543\n",
      "iteration: 385, cost: 0.32640892071936545\n",
      "iteration: 386, cost: 0.3261378207362985\n",
      "iteration: 387, cost: 0.32586741555358567\n",
      "iteration: 388, cost: 0.32559770181020126\n",
      "iteration: 389, cost: 0.3253286761689393\n",
      "iteration: 390, cost: 0.32506033531619366\n",
      "iteration: 391, cost: 0.3247926759617412\n",
      "iteration: 392, cost: 0.3245256948385267\n",
      "iteration: 393, cost: 0.32425938870245086\n",
      "iteration: 394, cost: 0.3239937543321599\n",
      "iteration: 395, cost: 0.32372878852883846\n",
      "iteration: 396, cost: 0.32346448811600365\n",
      "iteration: 397, cost: 0.323200849939303\n",
      "iteration: 398, cost: 0.3229378708663127\n",
      "iteration: 399, cost: 0.3226755477863398\n",
      "iteration: 400, cost: 0.3224138776102257\n",
      "iteration: 401, cost: 0.3221528572701521\n",
      "iteration: 402, cost: 0.32189248371944873\n",
      "iteration: 403, cost: 0.32163275393240376\n",
      "iteration: 404, cost: 0.3213736649040761\n",
      "iteration: 405, cost: 0.32111521365010937\n",
      "iteration: 406, cost: 0.32085739720654843\n",
      "iteration: 407, cost: 0.3206002126296578\n",
      "iteration: 408, cost: 0.32034365699574163\n",
      "iteration: 409, cost: 0.3200877274009664\n",
      "iteration: 410, cost: 0.3198324209611847\n",
      "iteration: 411, cost: 0.31957773481176177\n",
      "iteration: 412, cost: 0.319323666107403\n",
      "iteration: 413, cost: 0.31907021202198427\n",
      "iteration: 414, cost: 0.31881736974838326\n",
      "iteration: 415, cost: 0.3185651364983132\n",
      "iteration: 416, cost: 0.3183135095021581\n",
      "iteration: 417, cost: 0.3180624860088095\n",
      "iteration: 418, cost: 0.3178120632855058\n",
      "iteration: 419, cost: 0.3175622386176724\n",
      "iteration: 420, cost: 0.31731300930876394\n",
      "iteration: 421, cost: 0.31706437268010845\n",
      "iteration: 422, cost: 0.31681632607075244\n",
      "iteration: 423, cost: 0.3165688668373086\n",
      "iteration: 424, cost: 0.3163219923538045\n",
      "iteration: 425, cost: 0.3160757000115323\n",
      "iteration: 426, cost: 0.3158299872189019\n",
      "iteration: 427, cost: 0.31558485140129366\n",
      "iteration: 428, cost: 0.3153402900009135\n",
      "iteration: 429, cost: 0.31509630047664994\n",
      "iteration: 430, cost: 0.31485288030393166\n",
      "iteration: 431, cost: 0.3146100269745876\n",
      "iteration: 432, cost: 0.3143677379967074\n",
      "iteration: 433, cost: 0.31412601089450437\n",
      "iteration: 434, cost: 0.31388484320817917\n",
      "iteration: 435, cost: 0.31364423249378515\n",
      "iteration: 436, cost: 0.3134041763230951\n",
      "iteration: 437, cost: 0.3131646722834691\n",
      "iteration: 438, cost: 0.3129257179777245\n",
      "iteration: 439, cost: 0.3126873110240059\n",
      "iteration: 440, cost: 0.31244944905565797\n",
      "iteration: 441, cost: 0.3122121297210986\n",
      "iteration: 442, cost: 0.31197535068369336\n",
      "iteration: 443, cost: 0.31173910962163204\n",
      "iteration: 444, cost: 0.31150340422780515\n",
      "iteration: 445, cost: 0.31126823220968325\n",
      "iteration: 446, cost: 0.311033591289196\n",
      "iteration: 447, cost: 0.3107994792026133\n",
      "iteration: 448, cost: 0.3105658937004277\n",
      "iteration: 449, cost: 0.31033283254723737\n",
      "iteration: 450, cost: 0.3101002935216309\n",
      "iteration: 451, cost: 0.3098682744160727\n",
      "iteration: 452, cost: 0.30963677303678994\n",
      "iteration: 453, cost: 0.3094057872036606\n",
      "iteration: 454, cost: 0.30917531475010235\n",
      "iteration: 455, cost: 0.30894535352296293\n",
      "iteration: 456, cost: 0.3087159013824112\n",
      "iteration: 457, cost: 0.30848695620182964\n",
      "iteration: 458, cost: 0.3082585158677077\n",
      "iteration: 459, cost: 0.3080305782795362\n",
      "iteration: 460, cost: 0.30780314134970316\n",
      "iteration: 461, cost: 0.30757620300338995\n",
      "iteration: 462, cost: 0.307349761178469\n",
      "iteration: 463, cost: 0.30712381382540244\n",
      "iteration: 464, cost: 0.3068983589071417\n",
      "iteration: 465, cost: 0.30667339439902785\n",
      "iteration: 466, cost: 0.30644891828869325\n",
      "iteration: 467, cost: 0.3062249285759641\n",
      "iteration: 468, cost: 0.3060014232727639\n",
      "iteration: 469, cost: 0.30577840040301735\n",
      "iteration: 470, cost: 0.3055558580025564\n",
      "iteration: 471, cost: 0.305333794119026\n",
      "iteration: 472, cost: 0.3051122068117912\n",
      "iteration: 473, cost: 0.3048910941518453\n",
      "iteration: 474, cost: 0.3046704542217189\n",
      "iteration: 475, cost: 0.3044502851153893\n",
      "iteration: 476, cost: 0.30423058493819155\n",
      "iteration: 477, cost: 0.3040113518067297\n",
      "iteration: 478, cost: 0.30379258384878915\n",
      "iteration: 479, cost: 0.3035742792032501\n",
      "iteration: 480, cost: 0.3033564360200011\n",
      "iteration: 481, cost: 0.3031390524598546\n",
      "iteration: 482, cost: 0.3029221266944618\n",
      "iteration: 483, cost: 0.3027056569062296\n",
      "iteration: 484, cost: 0.3024896412882375\n",
      "iteration: 485, cost: 0.30227407804415624\n",
      "iteration: 486, cost: 0.30205896538816573\n",
      "iteration: 487, cost: 0.30184430154487524\n",
      "iteration: 488, cost: 0.3016300847492436\n",
      "iteration: 489, cost: 0.3014163132465\n",
      "iteration: 490, cost: 0.301202985292066\n",
      "iteration: 491, cost: 0.3009900991514782\n",
      "iteration: 492, cost: 0.300777653100311\n",
      "iteration: 493, cost: 0.30056564542410114\n",
      "iteration: 494, cost: 0.3003540744182721\n",
      "iteration: 495, cost: 0.3001429383880594\n",
      "iteration: 496, cost: 0.29993223564843696\n",
      "iteration: 497, cost: 0.2997219645240435\n",
      "iteration: 498, cost: 0.2995121233491101\n",
      "iteration: 499, cost: 0.29930271046738866\n"
     ]
    }
   ],
   "source": [
    "W,b = training_phase(X_train1, Y_train, 10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_phase(W, b, X):\n",
    "    Y_predict=np.zeros((1,X.shape[0]))\n",
    "    A = sigmoid(np.dot(W.T,X.T)+b)\n",
    "    for i in range(A.shape[1]):\n",
    "        if(A[0,i]>0.5):\n",
    "            Y_predict[0,i] = 1\n",
    "    return Y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = testing_phase(W,b,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. ... 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8607414448669202"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_predict.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
