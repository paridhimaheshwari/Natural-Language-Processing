{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-q50lzG5wyqs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub7nzv5nxc7Y"
      },
      "source": [
        "**Step 0: Preparing data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gcJZc9bvw_jM",
        "outputId": "1d0f0cbb-e12a-4d20-8977-3f4868d73d8a"
      },
      "outputs": [],
      "source": [
        "training_data=torchvision.datasets.MNIST(root='./data',\n",
        "                                         train=True,\n",
        "                                         transform=torchvision.transforms.ToTensor(),\n",
        "                                         download=True\n",
        "                                         )\n",
        "test_data=torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     transform=torchvision.transforms.ToTensor()\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T50fXq0AylBs",
        "outputId": "fcad7e4a-9446-4ce8-8022-e64b4b7f8661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot8b6Jqw0Zja"
      },
      "source": [
        "**Splitting Data in batches using DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hyt-rvJDynzP"
      },
      "outputs": [],
      "source": [
        "train_loader=torch.utils.data.DataLoader(dataset=training_data,\n",
        "                                         batch_size=128,\n",
        "                                         num_workers=2\n",
        "                                         )\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                         batch_size=128,\n",
        "                                         num_workers=2\n",
        "                                         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns5IoixB0M6e"
      },
      "source": [
        "**Getting one batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x9NzrgO_zRrC"
      },
      "outputs": [],
      "source": [
        "example=iter(train_loader)\n",
        "image,label=next(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUMR-fIs0SnE"
      },
      "source": [
        "**Plotting Few images of First Batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "eW--tao4zcFE",
        "outputId": "bd815298-9706-4366-d120-7024d5eb08e4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(image[i][0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-zIkDtg0kgf"
      },
      "source": [
        "**Setting Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lyD4kkwp0Hgv"
      },
      "outputs": [],
      "source": [
        "num_epochs=3\n",
        "learning_rate=0.001\n",
        "input_size=784 #(28*28)\n",
        "hidden_units=500 #(hidden layer 1: number of units)\n",
        "output_size=10 #(10 unique labels for number of digits from 0-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk4Jmo-F1KBA"
      },
      "source": [
        "**Step 1: Defining Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q_UAmA-j3LgJ"
      },
      "outputs": [],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w4KMEe4O1Ja7"
      },
      "outputs": [],
      "source": [
        "class neural_nn(nn.Module):\n",
        "  def __init__(self,input_size,hidden_units,output_size):\n",
        "    super(neural_nn,self).__init__()\n",
        "    self.input_size=input_size\n",
        "    self.l1=nn.Linear(input_size,hidden_units)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.l2=nn.Linear(hidden_units,output_size)\n",
        "  def forward(self,x):\n",
        "    out=self.l1(x)\n",
        "    out=self.relu(out)\n",
        "    out=self.l2(out)\n",
        "    return out\n",
        "model=neural_nn(input_size,hidden_units,output_size).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFu1NX9n37s1"
      },
      "source": [
        "**Step2: optimizer and Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1BErrw1h3XEd"
      },
      "outputs": [],
      "source": [
        "optimizer=torch.optim.Adam(params=model.parameters(),lr=learning_rate)\n",
        "criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VADO1zVc4d1y"
      },
      "source": [
        "**Training the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9hGP8Lt4c9T",
        "outputId": "b064bd28-2a24-4e04-a76c-6ff02ecdc5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration:0 step:0 loss:0.05361231043934822\n",
            "iteration:0 step:1 loss:0.11463505029678345\n",
            "iteration:0 step:2 loss:0.03350849449634552\n",
            "iteration:0 step:3 loss:0.1470109522342682\n",
            "iteration:0 step:4 loss:0.05977089703083038\n",
            "iteration:0 step:5 loss:0.11525452882051468\n",
            "iteration:0 step:6 loss:0.09099026024341583\n",
            "iteration:0 step:7 loss:0.07683011144399643\n",
            "iteration:0 step:8 loss:0.19908230006694794\n",
            "iteration:0 step:9 loss:0.13525766134262085\n",
            "iteration:0 step:10 loss:0.11802580207586288\n",
            "iteration:0 step:11 loss:0.0386311374604702\n",
            "iteration:0 step:12 loss:0.06742075830698013\n",
            "iteration:0 step:13 loss:0.05403180420398712\n",
            "iteration:0 step:14 loss:0.02405981905758381\n",
            "iteration:0 step:15 loss:0.07095421850681305\n",
            "iteration:0 step:16 loss:0.034529998898506165\n",
            "iteration:0 step:17 loss:0.041976332664489746\n",
            "iteration:0 step:18 loss:0.050677742809057236\n",
            "iteration:0 step:19 loss:0.02587233856320381\n",
            "iteration:0 step:20 loss:0.10209948569536209\n",
            "iteration:0 step:21 loss:0.07617934048175812\n",
            "iteration:0 step:22 loss:0.03889995813369751\n",
            "iteration:0 step:23 loss:0.06444627791643143\n",
            "iteration:0 step:24 loss:0.018661564216017723\n",
            "iteration:0 step:25 loss:0.04592876508831978\n",
            "iteration:0 step:26 loss:0.034163475036621094\n",
            "iteration:0 step:27 loss:0.0570988692343235\n",
            "iteration:0 step:28 loss:0.06878788024187088\n",
            "iteration:0 step:29 loss:0.05182476341724396\n",
            "iteration:0 step:30 loss:0.020640196278691292\n",
            "iteration:0 step:31 loss:0.06309407949447632\n",
            "iteration:0 step:32 loss:0.0480552576482296\n",
            "iteration:0 step:33 loss:0.07468341290950775\n",
            "iteration:0 step:34 loss:0.061707623302936554\n",
            "iteration:0 step:35 loss:0.026711301878094673\n",
            "iteration:0 step:36 loss:0.0918903574347496\n",
            "iteration:0 step:37 loss:0.027938785031437874\n",
            "iteration:0 step:38 loss:0.07270164787769318\n",
            "iteration:0 step:39 loss:0.09453151375055313\n",
            "iteration:0 step:40 loss:0.073075070977211\n",
            "iteration:0 step:41 loss:0.07264780253171921\n",
            "iteration:0 step:42 loss:0.02388210967183113\n",
            "iteration:0 step:43 loss:0.05415188521146774\n",
            "iteration:0 step:44 loss:0.07582029700279236\n",
            "iteration:0 step:45 loss:0.0932396724820137\n",
            "iteration:0 step:46 loss:0.05919630452990532\n",
            "iteration:0 step:47 loss:0.04991723597049713\n",
            "iteration:0 step:48 loss:0.05556720867753029\n",
            "iteration:0 step:49 loss:0.024245521053671837\n",
            "iteration:0 step:50 loss:0.038066886365413666\n",
            "iteration:0 step:51 loss:0.037721388041973114\n",
            "iteration:0 step:52 loss:0.03912780061364174\n",
            "iteration:0 step:53 loss:0.1681959629058838\n",
            "iteration:0 step:54 loss:0.11700595915317535\n",
            "iteration:0 step:55 loss:0.08438671380281448\n",
            "iteration:0 step:56 loss:0.09204316139221191\n",
            "iteration:0 step:57 loss:0.06799241155385971\n",
            "iteration:0 step:58 loss:0.054132141172885895\n",
            "iteration:0 step:59 loss:0.07529683411121368\n",
            "iteration:0 step:60 loss:0.053195588290691376\n",
            "iteration:0 step:61 loss:0.08397291600704193\n",
            "iteration:0 step:62 loss:0.05136888474225998\n",
            "iteration:0 step:63 loss:0.07193045318126678\n",
            "iteration:0 step:64 loss:0.18469378352165222\n",
            "iteration:0 step:65 loss:0.045405063778162\n",
            "iteration:0 step:66 loss:0.07820586860179901\n",
            "iteration:0 step:67 loss:0.06749621033668518\n",
            "iteration:0 step:68 loss:0.18560263514518738\n",
            "iteration:0 step:69 loss:0.150076761841774\n",
            "iteration:0 step:70 loss:0.06068825349211693\n",
            "iteration:0 step:71 loss:0.05177890881896019\n",
            "iteration:0 step:72 loss:0.10723723471164703\n",
            "iteration:0 step:73 loss:0.07120269536972046\n",
            "iteration:0 step:74 loss:0.042448170483112335\n",
            "iteration:0 step:75 loss:0.04432555288076401\n",
            "iteration:0 step:76 loss:0.06085740029811859\n",
            "iteration:0 step:77 loss:0.046081092208623886\n",
            "iteration:0 step:78 loss:0.058151792734861374\n",
            "iteration:0 step:79 loss:0.10555857419967651\n",
            "iteration:0 step:80 loss:0.10126715153455734\n",
            "iteration:0 step:81 loss:0.029735112562775612\n",
            "iteration:0 step:82 loss:0.013264661654829979\n",
            "iteration:0 step:83 loss:0.04336395859718323\n",
            "iteration:0 step:84 loss:0.051357436925172806\n",
            "iteration:0 step:85 loss:0.06394688785076141\n",
            "iteration:0 step:86 loss:0.03504348546266556\n",
            "iteration:0 step:87 loss:0.06516844779253006\n",
            "iteration:0 step:88 loss:0.02309461496770382\n",
            "iteration:0 step:89 loss:0.03603612259030342\n",
            "iteration:0 step:90 loss:0.1040753424167633\n",
            "iteration:0 step:91 loss:0.11357221752405167\n",
            "iteration:0 step:92 loss:0.10134241729974747\n",
            "iteration:0 step:93 loss:0.09353220462799072\n",
            "iteration:0 step:94 loss:0.026729639619588852\n",
            "iteration:0 step:95 loss:0.0434875451028347\n",
            "iteration:0 step:96 loss:0.07717134803533554\n",
            "iteration:0 step:97 loss:0.06993831694126129\n",
            "iteration:0 step:98 loss:0.17801566421985626\n",
            "iteration:0 step:99 loss:0.08650339394807816\n",
            "iteration:0 step:100 loss:0.041377365589141846\n",
            "iteration:0 step:101 loss:0.07847532629966736\n",
            "iteration:0 step:102 loss:0.05298849940299988\n",
            "iteration:0 step:103 loss:0.06322076171636581\n",
            "iteration:0 step:104 loss:0.05772710219025612\n",
            "iteration:0 step:105 loss:0.05107499286532402\n",
            "iteration:0 step:106 loss:0.08617955446243286\n",
            "iteration:0 step:107 loss:0.06711120158433914\n",
            "iteration:0 step:108 loss:0.0426122285425663\n",
            "iteration:0 step:109 loss:0.10589668899774551\n",
            "iteration:0 step:110 loss:0.05741429328918457\n",
            "iteration:0 step:111 loss:0.0829470306634903\n",
            "iteration:0 step:112 loss:0.0613899752497673\n",
            "iteration:0 step:113 loss:0.06405941396951675\n",
            "iteration:0 step:114 loss:0.049650780856609344\n",
            "iteration:0 step:115 loss:0.10667039453983307\n",
            "iteration:0 step:116 loss:0.026204289868474007\n",
            "iteration:0 step:117 loss:0.04296307638287544\n",
            "iteration:0 step:118 loss:0.07581966370344162\n",
            "iteration:0 step:119 loss:0.08679641038179398\n",
            "iteration:0 step:120 loss:0.05489778891205788\n",
            "iteration:0 step:121 loss:0.04607919603586197\n",
            "iteration:0 step:122 loss:0.052710939198732376\n",
            "iteration:0 step:123 loss:0.13594204187393188\n",
            "iteration:0 step:124 loss:0.08724165707826614\n",
            "iteration:0 step:125 loss:0.08742634207010269\n",
            "iteration:0 step:126 loss:0.053334057331085205\n",
            "iteration:0 step:127 loss:0.06525862216949463\n",
            "iteration:0 step:128 loss:0.07071516662836075\n",
            "iteration:0 step:129 loss:0.05638906732201576\n",
            "iteration:0 step:130 loss:0.07615027576684952\n",
            "iteration:0 step:131 loss:0.044792383909225464\n",
            "iteration:0 step:132 loss:0.06880252808332443\n",
            "iteration:0 step:133 loss:0.08943890780210495\n",
            "iteration:0 step:134 loss:0.07743275910615921\n",
            "iteration:0 step:135 loss:0.037040531635284424\n",
            "iteration:0 step:136 loss:0.048884712159633636\n",
            "iteration:0 step:137 loss:0.06023212894797325\n",
            "iteration:0 step:138 loss:0.12368778139352798\n",
            "iteration:0 step:139 loss:0.08155804127454758\n",
            "iteration:0 step:140 loss:0.05697615072131157\n",
            "iteration:0 step:141 loss:0.06429325044155121\n",
            "iteration:0 step:142 loss:0.035633448511362076\n",
            "iteration:0 step:143 loss:0.055863261222839355\n",
            "iteration:0 step:144 loss:0.0538233183324337\n",
            "iteration:0 step:145 loss:0.04782449081540108\n",
            "iteration:0 step:146 loss:0.03799192234873772\n",
            "iteration:0 step:147 loss:0.01635565795004368\n",
            "iteration:0 step:148 loss:0.031696755439043045\n",
            "iteration:0 step:149 loss:0.09376729279756546\n",
            "iteration:0 step:150 loss:0.03285757079720497\n",
            "iteration:0 step:151 loss:0.10843752324581146\n",
            "iteration:0 step:152 loss:0.07332874834537506\n",
            "iteration:0 step:153 loss:0.04504700005054474\n",
            "iteration:0 step:154 loss:0.062086816877126694\n",
            "iteration:0 step:155 loss:0.07576337456703186\n",
            "iteration:0 step:156 loss:0.08374252915382385\n",
            "iteration:0 step:157 loss:0.06109379604458809\n",
            "iteration:0 step:158 loss:0.06407973915338516\n",
            "iteration:0 step:159 loss:0.01641714759171009\n",
            "iteration:0 step:160 loss:0.05485011264681816\n",
            "iteration:0 step:161 loss:0.14501693844795227\n",
            "iteration:0 step:162 loss:0.145025372505188\n",
            "iteration:0 step:163 loss:0.08622067421674728\n",
            "iteration:0 step:164 loss:0.06921110302209854\n",
            "iteration:0 step:165 loss:0.05414850637316704\n",
            "iteration:0 step:166 loss:0.036296337842941284\n",
            "iteration:0 step:167 loss:0.03328392654657364\n",
            "iteration:0 step:168 loss:0.1393837034702301\n",
            "iteration:0 step:169 loss:0.034559838473796844\n",
            "iteration:0 step:170 loss:0.0186003427952528\n",
            "iteration:0 step:171 loss:0.08988495916128159\n",
            "iteration:0 step:172 loss:0.03931938484311104\n",
            "iteration:0 step:173 loss:0.10915475338697433\n",
            "iteration:0 step:174 loss:0.04656948894262314\n",
            "iteration:0 step:175 loss:0.10146909207105637\n",
            "iteration:0 step:176 loss:0.1303025782108307\n",
            "iteration:0 step:177 loss:0.049156274646520615\n",
            "iteration:0 step:178 loss:0.04054689779877663\n",
            "iteration:0 step:179 loss:0.03442150354385376\n",
            "iteration:0 step:180 loss:0.04999811574816704\n",
            "iteration:0 step:181 loss:0.059239696711301804\n",
            "iteration:0 step:182 loss:0.07611560076475143\n",
            "iteration:0 step:183 loss:0.031691040843725204\n",
            "iteration:0 step:184 loss:0.10685056447982788\n",
            "iteration:0 step:185 loss:0.07277067005634308\n",
            "iteration:0 step:186 loss:0.08202940225601196\n",
            "iteration:0 step:187 loss:0.10570725798606873\n",
            "iteration:0 step:188 loss:0.04201361536979675\n",
            "iteration:0 step:189 loss:0.09043662250041962\n",
            "iteration:0 step:190 loss:0.009687255136668682\n",
            "iteration:0 step:191 loss:0.0825926810503006\n",
            "iteration:0 step:192 loss:0.05031068995594978\n",
            "iteration:0 step:193 loss:0.08490514010190964\n",
            "iteration:0 step:194 loss:0.05801444128155708\n",
            "iteration:0 step:195 loss:0.013457078486680984\n",
            "iteration:0 step:196 loss:0.0892086923122406\n",
            "iteration:0 step:197 loss:0.0670323371887207\n",
            "iteration:0 step:198 loss:0.012470368295907974\n",
            "iteration:0 step:199 loss:0.12503138184547424\n",
            "iteration:0 step:200 loss:0.060747891664505005\n",
            "iteration:0 step:201 loss:0.08663387596607208\n",
            "iteration:0 step:202 loss:0.03922406956553459\n",
            "iteration:0 step:203 loss:0.03724715858697891\n",
            "iteration:0 step:204 loss:0.027210688218474388\n",
            "iteration:0 step:205 loss:0.05573029816150665\n",
            "iteration:0 step:206 loss:0.12447261065244675\n",
            "iteration:0 step:207 loss:0.1523737758398056\n",
            "iteration:0 step:208 loss:0.1480286568403244\n",
            "iteration:0 step:209 loss:0.060796089470386505\n",
            "iteration:0 step:210 loss:0.0954657793045044\n",
            "iteration:0 step:211 loss:0.02624642848968506\n",
            "iteration:0 step:212 loss:0.18577580153942108\n",
            "iteration:0 step:213 loss:0.01961260475218296\n",
            "iteration:0 step:214 loss:0.05869745463132858\n",
            "iteration:0 step:215 loss:0.07487951219081879\n",
            "iteration:0 step:216 loss:0.06253723055124283\n",
            "iteration:0 step:217 loss:0.11366107314825058\n",
            "iteration:0 step:218 loss:0.018901515752077103\n",
            "iteration:0 step:219 loss:0.04106488078832626\n",
            "iteration:0 step:220 loss:0.05821941792964935\n",
            "iteration:0 step:221 loss:0.11759039014577866\n",
            "iteration:0 step:222 loss:0.038298994302749634\n",
            "iteration:0 step:223 loss:0.09445444494485855\n",
            "iteration:0 step:224 loss:0.06091500073671341\n",
            "iteration:0 step:225 loss:0.0416487455368042\n",
            "iteration:0 step:226 loss:0.0680975541472435\n",
            "iteration:0 step:227 loss:0.045775461941957474\n",
            "iteration:0 step:228 loss:0.0793905258178711\n",
            "iteration:0 step:229 loss:0.0631365031003952\n",
            "iteration:0 step:230 loss:0.03353032097220421\n",
            "iteration:0 step:231 loss:0.05321110039949417\n",
            "iteration:0 step:232 loss:0.054866060614585876\n",
            "iteration:0 step:233 loss:0.07568136602640152\n",
            "iteration:0 step:234 loss:0.08830083161592484\n",
            "iteration:0 step:235 loss:0.06843788921833038\n",
            "iteration:0 step:236 loss:0.024009061977267265\n",
            "iteration:0 step:237 loss:0.028837254270911217\n",
            "iteration:0 step:238 loss:0.061606697738170624\n",
            "iteration:0 step:239 loss:0.06306152790784836\n",
            "iteration:0 step:240 loss:0.05291106924414635\n",
            "iteration:0 step:241 loss:0.0779159739613533\n",
            "iteration:0 step:242 loss:0.036345116794109344\n",
            "iteration:0 step:243 loss:0.11494611948728561\n",
            "iteration:0 step:244 loss:0.07310676574707031\n",
            "iteration:0 step:245 loss:0.048566557466983795\n",
            "iteration:0 step:246 loss:0.09077715873718262\n",
            "iteration:0 step:247 loss:0.0979418009519577\n",
            "iteration:0 step:248 loss:0.08205921202898026\n",
            "iteration:0 step:249 loss:0.04148169979453087\n",
            "iteration:0 step:250 loss:0.08911919593811035\n",
            "iteration:0 step:251 loss:0.05664518103003502\n",
            "iteration:0 step:252 loss:0.07933137565851212\n",
            "iteration:0 step:253 loss:0.12178714573383331\n",
            "iteration:0 step:254 loss:0.03273339942097664\n",
            "iteration:0 step:255 loss:0.09192713350057602\n",
            "iteration:0 step:256 loss:0.07257097214460373\n",
            "iteration:0 step:257 loss:0.023408453911542892\n",
            "iteration:0 step:258 loss:0.03362641483545303\n",
            "iteration:0 step:259 loss:0.044990796595811844\n",
            "iteration:0 step:260 loss:0.09363564103841782\n",
            "iteration:0 step:261 loss:0.06954677402973175\n",
            "iteration:0 step:262 loss:0.03390286862850189\n",
            "iteration:0 step:263 loss:0.07508158683776855\n",
            "iteration:0 step:264 loss:0.017753414809703827\n",
            "iteration:0 step:265 loss:0.03530061990022659\n",
            "iteration:0 step:266 loss:0.0686519518494606\n",
            "iteration:0 step:267 loss:0.017237771302461624\n",
            "iteration:0 step:268 loss:0.11106497049331665\n",
            "iteration:0 step:269 loss:0.053770750761032104\n",
            "iteration:0 step:270 loss:0.0862320140004158\n",
            "iteration:0 step:271 loss:0.10292503982782364\n",
            "iteration:0 step:272 loss:0.099067322909832\n",
            "iteration:0 step:273 loss:0.06354441493749619\n",
            "iteration:0 step:274 loss:0.043304443359375\n",
            "iteration:0 step:275 loss:0.07765046507120132\n",
            "iteration:0 step:276 loss:0.03939484804868698\n",
            "iteration:0 step:277 loss:0.09650570154190063\n",
            "iteration:0 step:278 loss:0.08281819522380829\n",
            "iteration:0 step:279 loss:0.017248287796974182\n",
            "iteration:0 step:280 loss:0.05098604038357735\n",
            "iteration:0 step:281 loss:0.06795338541269302\n",
            "iteration:0 step:282 loss:0.12683281302452087\n",
            "iteration:0 step:283 loss:0.023357408121228218\n",
            "iteration:0 step:284 loss:0.08841516822576523\n",
            "iteration:0 step:285 loss:0.02254730835556984\n",
            "iteration:0 step:286 loss:0.03808099776506424\n",
            "iteration:0 step:287 loss:0.06298601627349854\n",
            "iteration:0 step:288 loss:0.05575263127684593\n",
            "iteration:0 step:289 loss:0.12203637510538101\n",
            "iteration:0 step:290 loss:0.045953501015901566\n",
            "iteration:0 step:291 loss:0.08953669667243958\n",
            "iteration:0 step:292 loss:0.17897823452949524\n",
            "iteration:0 step:293 loss:0.030258696526288986\n",
            "iteration:0 step:294 loss:0.05560199171304703\n",
            "iteration:0 step:295 loss:0.12819001078605652\n",
            "iteration:0 step:296 loss:0.030829207971692085\n",
            "iteration:0 step:297 loss:0.026009533554315567\n",
            "iteration:0 step:298 loss:0.03040372021496296\n",
            "iteration:0 step:299 loss:0.08335892856121063\n",
            "iteration:0 step:300 loss:0.04073183611035347\n",
            "iteration:0 step:301 loss:0.07718802988529205\n",
            "iteration:0 step:302 loss:0.05594030022621155\n",
            "iteration:0 step:303 loss:0.013354938477277756\n",
            "iteration:0 step:304 loss:0.07540073245763779\n",
            "iteration:0 step:305 loss:0.018446603789925575\n",
            "iteration:0 step:306 loss:0.08335166424512863\n",
            "iteration:0 step:307 loss:0.16620473563671112\n",
            "iteration:0 step:308 loss:0.04562220722436905\n",
            "iteration:0 step:309 loss:0.041023094207048416\n",
            "iteration:0 step:310 loss:0.055054131895303726\n",
            "iteration:0 step:311 loss:0.061468496918678284\n",
            "iteration:0 step:312 loss:0.0552835613489151\n",
            "iteration:0 step:313 loss:0.04534874111413956\n",
            "iteration:0 step:314 loss:0.05040756240487099\n",
            "iteration:0 step:315 loss:0.05173974484205246\n",
            "iteration:0 step:316 loss:0.020635273307561874\n",
            "iteration:0 step:317 loss:0.04901545122265816\n",
            "iteration:0 step:318 loss:0.06724248081445694\n",
            "iteration:0 step:319 loss:0.04066002741456032\n",
            "iteration:0 step:320 loss:0.0837676003575325\n",
            "iteration:0 step:321 loss:0.055230364203453064\n",
            "iteration:0 step:322 loss:0.12386824190616608\n",
            "iteration:0 step:323 loss:0.07846486568450928\n",
            "iteration:0 step:324 loss:0.10201402008533478\n",
            "iteration:0 step:325 loss:0.030748384073376656\n",
            "iteration:0 step:326 loss:0.05185163393616676\n",
            "iteration:0 step:327 loss:0.08329673111438751\n",
            "iteration:0 step:328 loss:0.03755722567439079\n",
            "iteration:0 step:329 loss:0.04904678836464882\n",
            "iteration:0 step:330 loss:0.08014219254255295\n",
            "iteration:0 step:331 loss:0.06942347437143326\n",
            "iteration:0 step:332 loss:0.11584345996379852\n",
            "iteration:0 step:333 loss:0.031020620837807655\n",
            "iteration:0 step:334 loss:0.0830463320016861\n",
            "iteration:0 step:335 loss:0.0788811445236206\n",
            "iteration:0 step:336 loss:0.10951308906078339\n",
            "iteration:0 step:337 loss:0.032048359513282776\n",
            "iteration:0 step:338 loss:0.016151750460267067\n",
            "iteration:0 step:339 loss:0.045271195471286774\n",
            "iteration:0 step:340 loss:0.05268726870417595\n",
            "iteration:0 step:341 loss:0.07672163099050522\n",
            "iteration:0 step:342 loss:0.07284431904554367\n",
            "iteration:0 step:343 loss:0.05544079840183258\n",
            "iteration:0 step:344 loss:0.06637502461671829\n",
            "iteration:0 step:345 loss:0.03135412558913231\n",
            "iteration:0 step:346 loss:0.08045635372400284\n",
            "iteration:0 step:347 loss:0.11561361700296402\n",
            "iteration:0 step:348 loss:0.06824629008769989\n",
            "iteration:0 step:349 loss:0.026270871981978416\n",
            "iteration:0 step:350 loss:0.06533010303974152\n",
            "iteration:0 step:351 loss:0.0567423552274704\n",
            "iteration:0 step:352 loss:0.05787210538983345\n",
            "iteration:0 step:353 loss:0.07026726752519608\n",
            "iteration:0 step:354 loss:0.04856555163860321\n",
            "iteration:0 step:355 loss:0.08171330392360687\n",
            "iteration:0 step:356 loss:0.10965157300233841\n",
            "iteration:0 step:357 loss:0.061616502702236176\n",
            "iteration:0 step:358 loss:0.08094396442174911\n",
            "iteration:0 step:359 loss:0.08303508162498474\n",
            "iteration:0 step:360 loss:0.0775199607014656\n",
            "iteration:0 step:361 loss:0.1389244794845581\n",
            "iteration:0 step:362 loss:0.07876719534397125\n",
            "iteration:0 step:363 loss:0.022163566201925278\n",
            "iteration:0 step:364 loss:0.02512943744659424\n",
            "iteration:0 step:365 loss:0.09493809938430786\n",
            "iteration:0 step:366 loss:0.04594666510820389\n",
            "iteration:0 step:367 loss:0.09763292223215103\n",
            "iteration:0 step:368 loss:0.051915887743234634\n",
            "iteration:0 step:369 loss:0.1284036487340927\n",
            "iteration:0 step:370 loss:0.10997273027896881\n",
            "iteration:0 step:371 loss:0.10951380431652069\n",
            "iteration:0 step:372 loss:0.05459083989262581\n",
            "iteration:0 step:373 loss:0.037695299834012985\n",
            "iteration:0 step:374 loss:0.08920815587043762\n",
            "iteration:0 step:375 loss:0.052591390907764435\n",
            "iteration:0 step:376 loss:0.016429295763373375\n",
            "iteration:0 step:377 loss:0.05074802786111832\n",
            "iteration:0 step:378 loss:0.031486768275499344\n",
            "iteration:0 step:379 loss:0.0584786981344223\n",
            "iteration:0 step:380 loss:0.028169846162199974\n",
            "iteration:0 step:381 loss:0.03684953227639198\n",
            "iteration:0 step:382 loss:0.2056335210800171\n",
            "iteration:0 step:383 loss:0.0484040305018425\n",
            "iteration:0 step:384 loss:0.0519341379404068\n",
            "iteration:0 step:385 loss:0.028295496478676796\n",
            "iteration:0 step:386 loss:0.07942972332239151\n",
            "iteration:0 step:387 loss:0.13385793566703796\n",
            "iteration:0 step:388 loss:0.03716063126921654\n",
            "iteration:0 step:389 loss:0.1219182014465332\n",
            "iteration:0 step:390 loss:0.03719445317983627\n",
            "iteration:0 step:391 loss:0.03846786916255951\n",
            "iteration:0 step:392 loss:0.08694621920585632\n",
            "iteration:0 step:393 loss:0.13676539063453674\n",
            "iteration:0 step:394 loss:0.07485493272542953\n",
            "iteration:0 step:395 loss:0.02881331369280815\n",
            "iteration:0 step:396 loss:0.04576121270656586\n",
            "iteration:0 step:397 loss:0.05012234300374985\n",
            "iteration:0 step:398 loss:0.023125920444726944\n",
            "iteration:0 step:399 loss:0.04818717762827873\n",
            "iteration:0 step:400 loss:0.095773845911026\n",
            "iteration:0 step:401 loss:0.040365055203437805\n",
            "iteration:0 step:402 loss:0.0463515967130661\n",
            "iteration:0 step:403 loss:0.05421191081404686\n",
            "iteration:0 step:404 loss:0.05435003712773323\n",
            "iteration:0 step:405 loss:0.027420464903116226\n",
            "iteration:0 step:406 loss:0.05871191993355751\n",
            "iteration:0 step:407 loss:0.07591725885868073\n",
            "iteration:0 step:408 loss:0.06471225619316101\n",
            "iteration:0 step:409 loss:0.04704316705465317\n",
            "iteration:0 step:410 loss:0.022826021537184715\n",
            "iteration:0 step:411 loss:0.05104118213057518\n",
            "iteration:0 step:412 loss:0.06026877835392952\n",
            "iteration:0 step:413 loss:0.12408298254013062\n",
            "iteration:0 step:414 loss:0.049758877605199814\n",
            "iteration:0 step:415 loss:0.05215880274772644\n",
            "iteration:0 step:416 loss:0.009175882674753666\n",
            "iteration:0 step:417 loss:0.05638260394334793\n",
            "iteration:0 step:418 loss:0.06291309744119644\n",
            "iteration:0 step:419 loss:0.08555226027965546\n",
            "iteration:0 step:420 loss:0.03223863244056702\n",
            "iteration:0 step:421 loss:0.060013316571712494\n",
            "iteration:0 step:422 loss:0.05365051329135895\n",
            "iteration:0 step:423 loss:0.05875740945339203\n",
            "iteration:0 step:424 loss:0.029703279957175255\n",
            "iteration:0 step:425 loss:0.05632768198847771\n",
            "iteration:0 step:426 loss:0.05325305089354515\n",
            "iteration:0 step:427 loss:0.013231641612946987\n",
            "iteration:0 step:428 loss:0.07393757998943329\n",
            "iteration:0 step:429 loss:0.045151736587285995\n",
            "iteration:0 step:430 loss:0.022792993113398552\n",
            "iteration:0 step:431 loss:0.05775255709886551\n",
            "iteration:0 step:432 loss:0.07745900750160217\n",
            "iteration:0 step:433 loss:0.06513382494449615\n",
            "iteration:0 step:434 loss:0.03874969482421875\n",
            "iteration:0 step:435 loss:0.08862711489200592\n",
            "iteration:0 step:436 loss:0.055550940334796906\n",
            "iteration:0 step:437 loss:0.04358869418501854\n",
            "iteration:0 step:438 loss:0.0430106557905674\n",
            "iteration:0 step:439 loss:0.05453505739569664\n",
            "iteration:0 step:440 loss:0.038172025233507156\n",
            "iteration:0 step:441 loss:0.054257042706012726\n",
            "iteration:0 step:442 loss:0.07470928877592087\n",
            "iteration:0 step:443 loss:0.04507254436612129\n",
            "iteration:0 step:444 loss:0.03572046011686325\n",
            "iteration:0 step:445 loss:0.05763384699821472\n",
            "iteration:0 step:446 loss:0.021227745339274406\n",
            "iteration:0 step:447 loss:0.06205063685774803\n",
            "iteration:0 step:448 loss:0.03709666430950165\n",
            "iteration:0 step:449 loss:0.03772879019379616\n",
            "iteration:0 step:450 loss:0.04753695800900459\n",
            "iteration:0 step:451 loss:0.05958988144993782\n",
            "iteration:0 step:452 loss:0.030352000147104263\n",
            "iteration:0 step:453 loss:0.06768421828746796\n",
            "iteration:0 step:454 loss:0.018727028742432594\n",
            "iteration:0 step:455 loss:0.01463796105235815\n",
            "iteration:0 step:456 loss:0.023027660325169563\n",
            "iteration:0 step:457 loss:0.008344386704266071\n",
            "iteration:0 step:458 loss:0.028554219752550125\n",
            "iteration:0 step:459 loss:0.08135668188333511\n",
            "iteration:0 step:460 loss:0.004872503224760294\n",
            "iteration:0 step:461 loss:0.004731400404125452\n",
            "iteration:0 step:462 loss:0.005556994117796421\n",
            "iteration:0 step:463 loss:0.09283878654241562\n",
            "iteration:0 step:464 loss:0.036302223801612854\n",
            "iteration:0 step:465 loss:0.00975564680993557\n",
            "iteration:0 step:466 loss:0.18939484655857086\n",
            "iteration:0 step:467 loss:0.008140487596392632\n",
            "iteration:0 step:468 loss:0.13501623272895813\n",
            "iteration:1 step:0 loss:0.041111674159765244\n",
            "iteration:1 step:1 loss:0.09877562522888184\n",
            "iteration:1 step:2 loss:0.020950796082615852\n",
            "iteration:1 step:3 loss:0.13302767276763916\n",
            "iteration:1 step:4 loss:0.04041522741317749\n",
            "iteration:1 step:5 loss:0.08507336676120758\n",
            "iteration:1 step:6 loss:0.06083934009075165\n",
            "iteration:1 step:7 loss:0.05027494207024574\n",
            "iteration:1 step:8 loss:0.13222332298755646\n",
            "iteration:1 step:9 loss:0.10836119204759598\n",
            "iteration:1 step:10 loss:0.09762854874134064\n",
            "iteration:1 step:11 loss:0.02754353918135166\n",
            "iteration:1 step:12 loss:0.053890399634838104\n",
            "iteration:1 step:13 loss:0.041304998099803925\n",
            "iteration:1 step:14 loss:0.01618899777531624\n",
            "iteration:1 step:15 loss:0.05672174319624901\n",
            "iteration:1 step:16 loss:0.021083472296595573\n",
            "iteration:1 step:17 loss:0.02358265593647957\n",
            "iteration:1 step:18 loss:0.037251006811857224\n",
            "iteration:1 step:19 loss:0.017722496762871742\n",
            "iteration:1 step:20 loss:0.0820552185177803\n",
            "iteration:1 step:21 loss:0.06195594370365143\n",
            "iteration:1 step:22 loss:0.024955974891781807\n",
            "iteration:1 step:23 loss:0.044167324900627136\n",
            "iteration:1 step:24 loss:0.011547396890819073\n",
            "iteration:1 step:25 loss:0.030246520414948463\n",
            "iteration:1 step:26 loss:0.023749327287077904\n",
            "iteration:1 step:27 loss:0.038032252341508865\n",
            "iteration:1 step:28 loss:0.04774199426174164\n",
            "iteration:1 step:29 loss:0.03898598998785019\n",
            "iteration:1 step:30 loss:0.019153881818056107\n",
            "iteration:1 step:31 loss:0.04159027710556984\n",
            "iteration:1 step:32 loss:0.029690351337194443\n",
            "iteration:1 step:33 loss:0.042975686490535736\n",
            "iteration:1 step:34 loss:0.05129031836986542\n",
            "iteration:1 step:35 loss:0.014904787763953209\n",
            "iteration:1 step:36 loss:0.07299608737230301\n",
            "iteration:1 step:37 loss:0.019412856549024582\n",
            "iteration:1 step:38 loss:0.04293191805481911\n",
            "iteration:1 step:39 loss:0.06901607662439346\n",
            "iteration:1 step:40 loss:0.05934896692633629\n",
            "iteration:1 step:41 loss:0.05604169890284538\n",
            "iteration:1 step:42 loss:0.014749767258763313\n",
            "iteration:1 step:43 loss:0.036458421498537064\n",
            "iteration:1 step:44 loss:0.06756054610013962\n",
            "iteration:1 step:45 loss:0.07910075038671494\n",
            "iteration:1 step:46 loss:0.04941370710730553\n",
            "iteration:1 step:47 loss:0.030086137354373932\n",
            "iteration:1 step:48 loss:0.04213188216090202\n",
            "iteration:1 step:49 loss:0.016634508967399597\n",
            "iteration:1 step:50 loss:0.019610140472650528\n",
            "iteration:1 step:51 loss:0.024443987756967545\n",
            "iteration:1 step:52 loss:0.02604876644909382\n",
            "iteration:1 step:53 loss:0.14135579764842987\n",
            "iteration:1 step:54 loss:0.0788014829158783\n",
            "iteration:1 step:55 loss:0.06696271151304245\n",
            "iteration:1 step:56 loss:0.07157612591981888\n",
            "iteration:1 step:57 loss:0.053878303617239\n",
            "iteration:1 step:58 loss:0.03735782951116562\n",
            "iteration:1 step:59 loss:0.06334946304559708\n",
            "iteration:1 step:60 loss:0.038437772542238235\n",
            "iteration:1 step:61 loss:0.05939667671918869\n",
            "iteration:1 step:62 loss:0.04021919518709183\n",
            "iteration:1 step:63 loss:0.043106645345687866\n",
            "iteration:1 step:64 loss:0.1498042345046997\n",
            "iteration:1 step:65 loss:0.03327463939785957\n",
            "iteration:1 step:66 loss:0.050554562360048294\n",
            "iteration:1 step:67 loss:0.04229772090911865\n",
            "iteration:1 step:68 loss:0.13069839775562286\n",
            "iteration:1 step:69 loss:0.11167792975902557\n",
            "iteration:1 step:70 loss:0.045091718435287476\n",
            "iteration:1 step:71 loss:0.03645453229546547\n",
            "iteration:1 step:72 loss:0.09064646065235138\n",
            "iteration:1 step:73 loss:0.04298052936792374\n",
            "iteration:1 step:74 loss:0.031186286360025406\n",
            "iteration:1 step:75 loss:0.03584669530391693\n",
            "iteration:1 step:76 loss:0.04428666830062866\n",
            "iteration:1 step:77 loss:0.02728426456451416\n",
            "iteration:1 step:78 loss:0.03503626957535744\n",
            "iteration:1 step:79 loss:0.07887265831232071\n",
            "iteration:1 step:80 loss:0.08561062067747116\n",
            "iteration:1 step:81 loss:0.02388971857726574\n",
            "iteration:1 step:82 loss:0.009251317009329796\n",
            "iteration:1 step:83 loss:0.02291714772582054\n",
            "iteration:1 step:84 loss:0.032372236251831055\n",
            "iteration:1 step:85 loss:0.053107138723134995\n",
            "iteration:1 step:86 loss:0.016172386705875397\n",
            "iteration:1 step:87 loss:0.054136015474796295\n",
            "iteration:1 step:88 loss:0.01647070236504078\n",
            "iteration:1 step:89 loss:0.020272957161068916\n",
            "iteration:1 step:90 loss:0.06803260743618011\n",
            "iteration:1 step:91 loss:0.07112295925617218\n",
            "iteration:1 step:92 loss:0.06815636157989502\n",
            "iteration:1 step:93 loss:0.07595251500606537\n",
            "iteration:1 step:94 loss:0.014167297631502151\n",
            "iteration:1 step:95 loss:0.027366137132048607\n",
            "iteration:1 step:96 loss:0.0446351021528244\n",
            "iteration:1 step:97 loss:0.042433999478816986\n",
            "iteration:1 step:98 loss:0.09645914286375046\n",
            "iteration:1 step:99 loss:0.050816528499126434\n",
            "iteration:1 step:100 loss:0.029778797179460526\n",
            "iteration:1 step:101 loss:0.05246591195464134\n",
            "iteration:1 step:102 loss:0.02787812612950802\n",
            "iteration:1 step:103 loss:0.036148205399513245\n",
            "iteration:1 step:104 loss:0.039970867335796356\n",
            "iteration:1 step:105 loss:0.034691981971263885\n",
            "iteration:1 step:106 loss:0.05194508284330368\n",
            "iteration:1 step:107 loss:0.04282541945576668\n",
            "iteration:1 step:108 loss:0.026974353939294815\n",
            "iteration:1 step:109 loss:0.06960592418909073\n",
            "iteration:1 step:110 loss:0.03389151766896248\n",
            "iteration:1 step:111 loss:0.054733894765377045\n",
            "iteration:1 step:112 loss:0.03881266340613365\n",
            "iteration:1 step:113 loss:0.04948236793279648\n",
            "iteration:1 step:114 loss:0.03350707143545151\n",
            "iteration:1 step:115 loss:0.07779596000909805\n",
            "iteration:1 step:116 loss:0.017951952293515205\n",
            "iteration:1 step:117 loss:0.033778462558984756\n",
            "iteration:1 step:118 loss:0.03725172206759453\n",
            "iteration:1 step:119 loss:0.0628669485449791\n",
            "iteration:1 step:120 loss:0.036379870027303696\n",
            "iteration:1 step:121 loss:0.032916437834501266\n",
            "iteration:1 step:122 loss:0.03322642296552658\n",
            "iteration:1 step:123 loss:0.12879706919193268\n",
            "iteration:1 step:124 loss:0.06363656371831894\n",
            "iteration:1 step:125 loss:0.06362315267324448\n",
            "iteration:1 step:126 loss:0.034621965140104294\n",
            "iteration:1 step:127 loss:0.05380354821681976\n",
            "iteration:1 step:128 loss:0.05694403499364853\n",
            "iteration:1 step:129 loss:0.03909405320882797\n",
            "iteration:1 step:130 loss:0.06447017937898636\n",
            "iteration:1 step:131 loss:0.02895914390683174\n",
            "iteration:1 step:132 loss:0.04580437019467354\n",
            "iteration:1 step:133 loss:0.07276049256324768\n",
            "iteration:1 step:134 loss:0.06689848005771637\n",
            "iteration:1 step:135 loss:0.027687884867191315\n",
            "iteration:1 step:136 loss:0.036300528794527054\n",
            "iteration:1 step:137 loss:0.03960762917995453\n",
            "iteration:1 step:138 loss:0.09651250392198563\n",
            "iteration:1 step:139 loss:0.06264094263315201\n",
            "iteration:1 step:140 loss:0.04156728833913803\n",
            "iteration:1 step:141 loss:0.0473690964281559\n",
            "iteration:1 step:142 loss:0.025955958291888237\n",
            "iteration:1 step:143 loss:0.04158477485179901\n",
            "iteration:1 step:144 loss:0.033065065741539\n",
            "iteration:1 step:145 loss:0.032714877277612686\n",
            "iteration:1 step:146 loss:0.030063245445489883\n",
            "iteration:1 step:147 loss:0.010339341126382351\n",
            "iteration:1 step:148 loss:0.02536124363541603\n",
            "iteration:1 step:149 loss:0.07314476370811462\n",
            "iteration:1 step:150 loss:0.025421857833862305\n",
            "iteration:1 step:151 loss:0.08383288234472275\n",
            "iteration:1 step:152 loss:0.06665163487195969\n",
            "iteration:1 step:153 loss:0.036308739334344864\n",
            "iteration:1 step:154 loss:0.04258595407009125\n",
            "iteration:1 step:155 loss:0.060092002153396606\n",
            "iteration:1 step:156 loss:0.06122545897960663\n",
            "iteration:1 step:157 loss:0.049279142171144485\n",
            "iteration:1 step:158 loss:0.05594850331544876\n",
            "iteration:1 step:159 loss:0.010471873916685581\n",
            "iteration:1 step:160 loss:0.03434443473815918\n",
            "iteration:1 step:161 loss:0.10876259952783585\n",
            "iteration:1 step:162 loss:0.09959334880113602\n",
            "iteration:1 step:163 loss:0.05258475989103317\n",
            "iteration:1 step:164 loss:0.045693036168813705\n",
            "iteration:1 step:165 loss:0.03940943256020546\n",
            "iteration:1 step:166 loss:0.02426273003220558\n",
            "iteration:1 step:167 loss:0.027567489072680473\n",
            "iteration:1 step:168 loss:0.11180384457111359\n",
            "iteration:1 step:169 loss:0.021912211552262306\n",
            "iteration:1 step:170 loss:0.009411345236003399\n",
            "iteration:1 step:171 loss:0.06723415106534958\n",
            "iteration:1 step:172 loss:0.023659398779273033\n",
            "iteration:1 step:173 loss:0.08131817728281021\n",
            "iteration:1 step:174 loss:0.037158239632844925\n",
            "iteration:1 step:175 loss:0.06981062144041061\n",
            "iteration:1 step:176 loss:0.09908537566661835\n",
            "iteration:1 step:177 loss:0.03529493138194084\n",
            "iteration:1 step:178 loss:0.021154776215553284\n",
            "iteration:1 step:179 loss:0.02035733312368393\n",
            "iteration:1 step:180 loss:0.029422612860798836\n",
            "iteration:1 step:181 loss:0.044589634984731674\n",
            "iteration:1 step:182 loss:0.0632750391960144\n",
            "iteration:1 step:183 loss:0.019224323332309723\n",
            "iteration:1 step:184 loss:0.0804278552532196\n",
            "iteration:1 step:185 loss:0.053534526377916336\n",
            "iteration:1 step:186 loss:0.07156253606081009\n",
            "iteration:1 step:187 loss:0.08203969150781631\n",
            "iteration:1 step:188 loss:0.03240848705172539\n",
            "iteration:1 step:189 loss:0.060355380177497864\n",
            "iteration:1 step:190 loss:0.006170549895614386\n",
            "iteration:1 step:191 loss:0.06175851821899414\n",
            "iteration:1 step:192 loss:0.03270711004734039\n",
            "iteration:1 step:193 loss:0.047161590307950974\n",
            "iteration:1 step:194 loss:0.04162157326936722\n",
            "iteration:1 step:195 loss:0.007692058105021715\n",
            "iteration:1 step:196 loss:0.07828903198242188\n",
            "iteration:1 step:197 loss:0.05375658720731735\n",
            "iteration:1 step:198 loss:0.007573676761239767\n",
            "iteration:1 step:199 loss:0.10724719613790512\n",
            "iteration:1 step:200 loss:0.043277740478515625\n",
            "iteration:1 step:201 loss:0.06775170564651489\n",
            "iteration:1 step:202 loss:0.02857263572514057\n",
            "iteration:1 step:203 loss:0.024751201272010803\n",
            "iteration:1 step:204 loss:0.022779513150453568\n",
            "iteration:1 step:205 loss:0.039933107793331146\n",
            "iteration:1 step:206 loss:0.09223054349422455\n",
            "iteration:1 step:207 loss:0.13446824252605438\n",
            "iteration:1 step:208 loss:0.12433873116970062\n",
            "iteration:1 step:209 loss:0.04138701409101486\n",
            "iteration:1 step:210 loss:0.0858599916100502\n",
            "iteration:1 step:211 loss:0.02100198157131672\n",
            "iteration:1 step:212 loss:0.1400548666715622\n",
            "iteration:1 step:213 loss:0.013615621253848076\n",
            "iteration:1 step:214 loss:0.046035006642341614\n",
            "iteration:1 step:215 loss:0.057202134281396866\n",
            "iteration:1 step:216 loss:0.04296320676803589\n",
            "iteration:1 step:217 loss:0.09502199292182922\n",
            "iteration:1 step:218 loss:0.013743970543146133\n",
            "iteration:1 step:219 loss:0.02585487626492977\n",
            "iteration:1 step:220 loss:0.044753167778253555\n",
            "iteration:1 step:221 loss:0.09268848598003387\n",
            "iteration:1 step:222 loss:0.025936415418982506\n",
            "iteration:1 step:223 loss:0.06191815063357353\n",
            "iteration:1 step:224 loss:0.048412080854177475\n",
            "iteration:1 step:225 loss:0.036025770008563995\n",
            "iteration:1 step:226 loss:0.05240059271454811\n",
            "iteration:1 step:227 loss:0.03218148648738861\n",
            "iteration:1 step:228 loss:0.059407174587249756\n",
            "iteration:1 step:229 loss:0.046146415174007416\n",
            "iteration:1 step:230 loss:0.029394760727882385\n",
            "iteration:1 step:231 loss:0.042212892323732376\n",
            "iteration:1 step:232 loss:0.0438365712761879\n",
            "iteration:1 step:233 loss:0.05331088602542877\n",
            "iteration:1 step:234 loss:0.05695108324289322\n",
            "iteration:1 step:235 loss:0.047294192016124725\n",
            "iteration:1 step:236 loss:0.015393172390758991\n",
            "iteration:1 step:237 loss:0.028438864275813103\n",
            "iteration:1 step:238 loss:0.04262590408325195\n",
            "iteration:1 step:239 loss:0.04541539400815964\n",
            "iteration:1 step:240 loss:0.03818357735872269\n",
            "iteration:1 step:241 loss:0.05798684060573578\n",
            "iteration:1 step:242 loss:0.025879954919219017\n",
            "iteration:1 step:243 loss:0.1061750054359436\n",
            "iteration:1 step:244 loss:0.04123901575803757\n",
            "iteration:1 step:245 loss:0.036584191024303436\n",
            "iteration:1 step:246 loss:0.06672558188438416\n",
            "iteration:1 step:247 loss:0.06236400455236435\n",
            "iteration:1 step:248 loss:0.0683722198009491\n",
            "iteration:1 step:249 loss:0.034511301666498184\n",
            "iteration:1 step:250 loss:0.06447271257638931\n",
            "iteration:1 step:251 loss:0.04306989163160324\n",
            "iteration:1 step:252 loss:0.05860011652112007\n",
            "iteration:1 step:253 loss:0.08469605445861816\n",
            "iteration:1 step:254 loss:0.025583285838365555\n",
            "iteration:1 step:255 loss:0.07393699139356613\n",
            "iteration:1 step:256 loss:0.038352854549884796\n",
            "iteration:1 step:257 loss:0.016331594437360764\n",
            "iteration:1 step:258 loss:0.022972209379076958\n",
            "iteration:1 step:259 loss:0.03074810653924942\n",
            "iteration:1 step:260 loss:0.07256809622049332\n",
            "iteration:1 step:261 loss:0.06194617971777916\n",
            "iteration:1 step:262 loss:0.02728196047246456\n",
            "iteration:1 step:263 loss:0.04188009351491928\n",
            "iteration:1 step:264 loss:0.011485286988317966\n",
            "iteration:1 step:265 loss:0.02581150084733963\n",
            "iteration:1 step:266 loss:0.053427983075380325\n",
            "iteration:1 step:267 loss:0.011195619590580463\n",
            "iteration:1 step:268 loss:0.09847265481948853\n",
            "iteration:1 step:269 loss:0.03648373484611511\n",
            "iteration:1 step:270 loss:0.06300712376832962\n",
            "iteration:1 step:271 loss:0.08893465250730515\n",
            "iteration:1 step:272 loss:0.07301007211208344\n",
            "iteration:1 step:273 loss:0.0496244803071022\n",
            "iteration:1 step:274 loss:0.03079076297581196\n",
            "iteration:1 step:275 loss:0.0644911378622055\n",
            "iteration:1 step:276 loss:0.028958089649677277\n",
            "iteration:1 step:277 loss:0.07516559213399887\n",
            "iteration:1 step:278 loss:0.0732027143239975\n",
            "iteration:1 step:279 loss:0.011457353830337524\n",
            "iteration:1 step:280 loss:0.03802964463829994\n",
            "iteration:1 step:281 loss:0.05665108188986778\n",
            "iteration:1 step:282 loss:0.10530421882867813\n",
            "iteration:1 step:283 loss:0.015844259411096573\n",
            "iteration:1 step:284 loss:0.07754091173410416\n",
            "iteration:1 step:285 loss:0.012467681430280209\n",
            "iteration:1 step:286 loss:0.025533074513077736\n",
            "iteration:1 step:287 loss:0.048841338604688644\n",
            "iteration:1 step:288 loss:0.03932954743504524\n",
            "iteration:1 step:289 loss:0.10055238753557205\n",
            "iteration:1 step:290 loss:0.026927094906568527\n",
            "iteration:1 step:291 loss:0.05614081770181656\n",
            "iteration:1 step:292 loss:0.12373443692922592\n",
            "iteration:1 step:293 loss:0.020760079845786095\n",
            "iteration:1 step:294 loss:0.03853781521320343\n",
            "iteration:1 step:295 loss:0.10729590058326721\n",
            "iteration:1 step:296 loss:0.019907336682081223\n",
            "iteration:1 step:297 loss:0.02113952860236168\n",
            "iteration:1 step:298 loss:0.027130598202347755\n",
            "iteration:1 step:299 loss:0.07251045852899551\n",
            "iteration:1 step:300 loss:0.02712683193385601\n",
            "iteration:1 step:301 loss:0.06873781234025955\n",
            "iteration:1 step:302 loss:0.047265201807022095\n",
            "iteration:1 step:303 loss:0.009509333409368992\n",
            "iteration:1 step:304 loss:0.06529500335454941\n",
            "iteration:1 step:305 loss:0.012341763824224472\n",
            "iteration:1 step:306 loss:0.06379078328609467\n",
            "iteration:1 step:307 loss:0.14058010280132294\n",
            "iteration:1 step:308 loss:0.033069826662540436\n",
            "iteration:1 step:309 loss:0.02721719816327095\n",
            "iteration:1 step:310 loss:0.04333465173840523\n",
            "iteration:1 step:311 loss:0.044315729290246964\n",
            "iteration:1 step:312 loss:0.036845043301582336\n",
            "iteration:1 step:313 loss:0.03526463359594345\n",
            "iteration:1 step:314 loss:0.0381351113319397\n",
            "iteration:1 step:315 loss:0.03626658767461777\n",
            "iteration:1 step:316 loss:0.012941493652760983\n",
            "iteration:1 step:317 loss:0.03332025930285454\n",
            "iteration:1 step:318 loss:0.05407419428229332\n",
            "iteration:1 step:319 loss:0.03271415829658508\n",
            "iteration:1 step:320 loss:0.06433309614658356\n",
            "iteration:1 step:321 loss:0.04265868663787842\n",
            "iteration:1 step:322 loss:0.09547322988510132\n",
            "iteration:1 step:323 loss:0.04922046512365341\n",
            "iteration:1 step:324 loss:0.08482081443071365\n",
            "iteration:1 step:325 loss:0.023057766258716583\n",
            "iteration:1 step:326 loss:0.034170038998126984\n",
            "iteration:1 step:327 loss:0.06768675148487091\n",
            "iteration:1 step:328 loss:0.02231682278215885\n",
            "iteration:1 step:329 loss:0.03430929407477379\n",
            "iteration:1 step:330 loss:0.0631004348397255\n",
            "iteration:1 step:331 loss:0.04157017171382904\n",
            "iteration:1 step:332 loss:0.09278423339128494\n",
            "iteration:1 step:333 loss:0.021000245586037636\n",
            "iteration:1 step:334 loss:0.06352928280830383\n",
            "iteration:1 step:335 loss:0.06601208448410034\n",
            "iteration:1 step:336 loss:0.10141076892614365\n",
            "iteration:1 step:337 loss:0.022855697199702263\n",
            "iteration:1 step:338 loss:0.01187889464199543\n",
            "iteration:1 step:339 loss:0.038219988346099854\n",
            "iteration:1 step:340 loss:0.0407591387629509\n",
            "iteration:1 step:341 loss:0.06428109109401703\n",
            "iteration:1 step:342 loss:0.055601879954338074\n",
            "iteration:1 step:343 loss:0.03760853409767151\n",
            "iteration:1 step:344 loss:0.05616363137960434\n",
            "iteration:1 step:345 loss:0.017586279660463333\n",
            "iteration:1 step:346 loss:0.06458697468042374\n",
            "iteration:1 step:347 loss:0.0953880026936531\n",
            "iteration:1 step:348 loss:0.05234849080443382\n",
            "iteration:1 step:349 loss:0.02016645111143589\n",
            "iteration:1 step:350 loss:0.042455267161130905\n",
            "iteration:1 step:351 loss:0.032723817974328995\n",
            "iteration:1 step:352 loss:0.049943696707487106\n",
            "iteration:1 step:353 loss:0.0563993975520134\n",
            "iteration:1 step:354 loss:0.0394614115357399\n",
            "iteration:1 step:355 loss:0.06563998013734818\n",
            "iteration:1 step:356 loss:0.06712144613265991\n",
            "iteration:1 step:357 loss:0.02918684296309948\n",
            "iteration:1 step:358 loss:0.05957737937569618\n",
            "iteration:1 step:359 loss:0.06415368616580963\n",
            "iteration:1 step:360 loss:0.04951723664999008\n",
            "iteration:1 step:361 loss:0.10309994220733643\n",
            "iteration:1 step:362 loss:0.04942663386464119\n",
            "iteration:1 step:363 loss:0.013820637948811054\n",
            "iteration:1 step:364 loss:0.018706489354372025\n",
            "iteration:1 step:365 loss:0.07557154446840286\n",
            "iteration:1 step:366 loss:0.03588416054844856\n",
            "iteration:1 step:367 loss:0.08096811920404434\n",
            "iteration:1 step:368 loss:0.0340353399515152\n",
            "iteration:1 step:369 loss:0.10590832680463791\n",
            "iteration:1 step:370 loss:0.08557397872209549\n",
            "iteration:1 step:371 loss:0.10015863180160522\n",
            "iteration:1 step:372 loss:0.03816066309809685\n",
            "iteration:1 step:373 loss:0.023282725363969803\n",
            "iteration:1 step:374 loss:0.07003633677959442\n",
            "iteration:1 step:375 loss:0.03815801441669464\n",
            "iteration:1 step:376 loss:0.011145754717290401\n",
            "iteration:1 step:377 loss:0.03472119942307472\n",
            "iteration:1 step:378 loss:0.022662730887532234\n",
            "iteration:1 step:379 loss:0.04579602926969528\n",
            "iteration:1 step:380 loss:0.018582265824079514\n",
            "iteration:1 step:381 loss:0.029221275821328163\n",
            "iteration:1 step:382 loss:0.16157634556293488\n",
            "iteration:1 step:383 loss:0.03806159645318985\n",
            "iteration:1 step:384 loss:0.04300421103835106\n",
            "iteration:1 step:385 loss:0.02327115833759308\n",
            "iteration:1 step:386 loss:0.06199324503540993\n",
            "iteration:1 step:387 loss:0.09697240591049194\n",
            "iteration:1 step:388 loss:0.02547481469810009\n",
            "iteration:1 step:389 loss:0.08887428790330887\n",
            "iteration:1 step:390 loss:0.028380077332258224\n",
            "iteration:1 step:391 loss:0.020315131172537804\n",
            "iteration:1 step:392 loss:0.0666654109954834\n",
            "iteration:1 step:393 loss:0.10144604742527008\n",
            "iteration:1 step:394 loss:0.05480024963617325\n",
            "iteration:1 step:395 loss:0.026176096871495247\n",
            "iteration:1 step:396 loss:0.037094082683324814\n",
            "iteration:1 step:397 loss:0.04502824321389198\n",
            "iteration:1 step:398 loss:0.01794813573360443\n",
            "iteration:1 step:399 loss:0.0425923727452755\n",
            "iteration:1 step:400 loss:0.0798802599310875\n",
            "iteration:1 step:401 loss:0.036390796303749084\n",
            "iteration:1 step:402 loss:0.027624772861599922\n",
            "iteration:1 step:403 loss:0.03181995823979378\n",
            "iteration:1 step:404 loss:0.03748999536037445\n",
            "iteration:1 step:405 loss:0.02035914920270443\n",
            "iteration:1 step:406 loss:0.043478190898895264\n",
            "iteration:1 step:407 loss:0.05601730942726135\n",
            "iteration:1 step:408 loss:0.04803738370537758\n",
            "iteration:1 step:409 loss:0.03597182407975197\n",
            "iteration:1 step:410 loss:0.01700218766927719\n",
            "iteration:1 step:411 loss:0.03826230391860008\n",
            "iteration:1 step:412 loss:0.04444911330938339\n",
            "iteration:1 step:413 loss:0.07842250168323517\n",
            "iteration:1 step:414 loss:0.04029432684183121\n",
            "iteration:1 step:415 loss:0.036710795015096664\n",
            "iteration:1 step:416 loss:0.006083352491259575\n",
            "iteration:1 step:417 loss:0.04375476390123367\n",
            "iteration:1 step:418 loss:0.030926939100027084\n",
            "iteration:1 step:419 loss:0.07282013446092606\n",
            "iteration:1 step:420 loss:0.017108147963881493\n",
            "iteration:1 step:421 loss:0.03929366171360016\n",
            "iteration:1 step:422 loss:0.044546741992235184\n",
            "iteration:1 step:423 loss:0.044484589248895645\n",
            "iteration:1 step:424 loss:0.01641831360757351\n",
            "iteration:1 step:425 loss:0.040321074426174164\n",
            "iteration:1 step:426 loss:0.043732449412345886\n",
            "iteration:1 step:427 loss:0.008496301248669624\n",
            "iteration:1 step:428 loss:0.06195409968495369\n",
            "iteration:1 step:429 loss:0.03374406322836876\n",
            "iteration:1 step:430 loss:0.014098738320171833\n",
            "iteration:1 step:431 loss:0.03967150300741196\n",
            "iteration:1 step:432 loss:0.06860531121492386\n",
            "iteration:1 step:433 loss:0.03490433841943741\n",
            "iteration:1 step:434 loss:0.028828559443354607\n",
            "iteration:1 step:435 loss:0.0663447231054306\n",
            "iteration:1 step:436 loss:0.037435002624988556\n",
            "iteration:1 step:437 loss:0.027436936274170876\n",
            "iteration:1 step:438 loss:0.030051367357373238\n",
            "iteration:1 step:439 loss:0.03947068005800247\n",
            "iteration:1 step:440 loss:0.02890862710773945\n",
            "iteration:1 step:441 loss:0.04151313751935959\n",
            "iteration:1 step:442 loss:0.05111629143357277\n",
            "iteration:1 step:443 loss:0.0364992581307888\n",
            "iteration:1 step:444 loss:0.01786956563591957\n",
            "iteration:1 step:445 loss:0.03536440059542656\n",
            "iteration:1 step:446 loss:0.016773777082562447\n",
            "iteration:1 step:447 loss:0.04016534611582756\n",
            "iteration:1 step:448 loss:0.03130574896931648\n",
            "iteration:1 step:449 loss:0.032709866762161255\n",
            "iteration:1 step:450 loss:0.03108404390513897\n",
            "iteration:1 step:451 loss:0.0570395328104496\n",
            "iteration:1 step:452 loss:0.022485332563519478\n",
            "iteration:1 step:453 loss:0.057578325271606445\n",
            "iteration:1 step:454 loss:0.016835110262036324\n",
            "iteration:1 step:455 loss:0.009791429154574871\n",
            "iteration:1 step:456 loss:0.018116334453225136\n",
            "iteration:1 step:457 loss:0.006827430799603462\n",
            "iteration:1 step:458 loss:0.01743411272764206\n",
            "iteration:1 step:459 loss:0.044721294194459915\n",
            "iteration:1 step:460 loss:0.0029148960020393133\n",
            "iteration:1 step:461 loss:0.0035141296684741974\n",
            "iteration:1 step:462 loss:0.0033217654563486576\n",
            "iteration:1 step:463 loss:0.057880859822034836\n",
            "iteration:1 step:464 loss:0.034205276519060135\n",
            "iteration:1 step:465 loss:0.007528482470661402\n",
            "iteration:1 step:466 loss:0.14580972492694855\n",
            "iteration:1 step:467 loss:0.006815322209149599\n",
            "iteration:1 step:468 loss:0.1086057797074318\n",
            "iteration:2 step:0 loss:0.03507747873663902\n",
            "iteration:2 step:1 loss:0.08566010743379593\n",
            "iteration:2 step:2 loss:0.014514283277094364\n",
            "iteration:2 step:3 loss:0.1120167225599289\n",
            "iteration:2 step:4 loss:0.028859538957476616\n",
            "iteration:2 step:5 loss:0.051115602254867554\n",
            "iteration:2 step:6 loss:0.04425926133990288\n",
            "iteration:2 step:7 loss:0.03450362756848335\n",
            "iteration:2 step:8 loss:0.08711403608322144\n",
            "iteration:2 step:9 loss:0.08565341681241989\n",
            "iteration:2 step:10 loss:0.08346660435199738\n",
            "iteration:2 step:11 loss:0.02020624466240406\n",
            "iteration:2 step:12 loss:0.04741799086332321\n",
            "iteration:2 step:13 loss:0.03702723979949951\n",
            "iteration:2 step:14 loss:0.012904946692287922\n",
            "iteration:2 step:15 loss:0.049943890422582626\n",
            "iteration:2 step:16 loss:0.013461540453135967\n",
            "iteration:2 step:17 loss:0.012952086515724659\n",
            "iteration:2 step:18 loss:0.029998542740941048\n",
            "iteration:2 step:19 loss:0.010637735016644001\n",
            "iteration:2 step:20 loss:0.06418292969465256\n",
            "iteration:2 step:21 loss:0.0491521880030632\n",
            "iteration:2 step:22 loss:0.018648261204361916\n",
            "iteration:2 step:23 loss:0.027846861630678177\n",
            "iteration:2 step:24 loss:0.007475055288523436\n",
            "iteration:2 step:25 loss:0.02077074535191059\n",
            "iteration:2 step:26 loss:0.015054753050208092\n",
            "iteration:2 step:27 loss:0.029380422085523605\n",
            "iteration:2 step:28 loss:0.03721567615866661\n",
            "iteration:2 step:29 loss:0.032581765204668045\n",
            "iteration:2 step:30 loss:0.016973700374364853\n",
            "iteration:2 step:31 loss:0.02784881740808487\n",
            "iteration:2 step:32 loss:0.01961330883204937\n",
            "iteration:2 step:33 loss:0.027292737737298012\n",
            "iteration:2 step:34 loss:0.04477373883128166\n",
            "iteration:2 step:35 loss:0.011834681034088135\n",
            "iteration:2 step:36 loss:0.0601964071393013\n",
            "iteration:2 step:37 loss:0.01752712018787861\n",
            "iteration:2 step:38 loss:0.02712848223745823\n",
            "iteration:2 step:39 loss:0.05002180486917496\n",
            "iteration:2 step:40 loss:0.04480510950088501\n",
            "iteration:2 step:41 loss:0.04169872775673866\n",
            "iteration:2 step:42 loss:0.01073535531759262\n",
            "iteration:2 step:43 loss:0.02802836149930954\n",
            "iteration:2 step:44 loss:0.054734967648983\n",
            "iteration:2 step:45 loss:0.05885506048798561\n",
            "iteration:2 step:46 loss:0.04707387834787369\n",
            "iteration:2 step:47 loss:0.020297762006521225\n",
            "iteration:2 step:48 loss:0.026990681886672974\n",
            "iteration:2 step:49 loss:0.012318252585828304\n",
            "iteration:2 step:50 loss:0.014199736528098583\n",
            "iteration:2 step:51 loss:0.015341982245445251\n",
            "iteration:2 step:52 loss:0.02250046841800213\n",
            "iteration:2 step:53 loss:0.12545186281204224\n",
            "iteration:2 step:54 loss:0.06475327908992767\n",
            "iteration:2 step:55 loss:0.048462286591529846\n",
            "iteration:2 step:56 loss:0.05693601071834564\n",
            "iteration:2 step:57 loss:0.043338898569345474\n",
            "iteration:2 step:58 loss:0.024691076949238777\n",
            "iteration:2 step:59 loss:0.05279045179486275\n",
            "iteration:2 step:60 loss:0.029356298968195915\n",
            "iteration:2 step:61 loss:0.03806444630026817\n",
            "iteration:2 step:62 loss:0.030556252226233482\n",
            "iteration:2 step:63 loss:0.028319045901298523\n",
            "iteration:2 step:64 loss:0.11576959490776062\n",
            "iteration:2 step:65 loss:0.03428216651082039\n",
            "iteration:2 step:66 loss:0.04262451454997063\n",
            "iteration:2 step:67 loss:0.025858735665678978\n",
            "iteration:2 step:68 loss:0.08167077600955963\n",
            "iteration:2 step:69 loss:0.07750134915113449\n",
            "iteration:2 step:70 loss:0.03637604042887688\n",
            "iteration:2 step:71 loss:0.030256271362304688\n",
            "iteration:2 step:72 loss:0.07580503076314926\n",
            "iteration:2 step:73 loss:0.02745671011507511\n",
            "iteration:2 step:74 loss:0.024399520829319954\n",
            "iteration:2 step:75 loss:0.02294636145234108\n",
            "iteration:2 step:76 loss:0.03244028612971306\n",
            "iteration:2 step:77 loss:0.014640621840953827\n",
            "iteration:2 step:78 loss:0.023000970482826233\n",
            "iteration:2 step:79 loss:0.05815723165869713\n",
            "iteration:2 step:80 loss:0.06028098985552788\n",
            "iteration:2 step:81 loss:0.016888851299881935\n",
            "iteration:2 step:82 loss:0.0058603789657354355\n",
            "iteration:2 step:83 loss:0.012940219603478909\n",
            "iteration:2 step:84 loss:0.023030508309602737\n",
            "iteration:2 step:85 loss:0.04522527754306793\n",
            "iteration:2 step:86 loss:0.011899981647729874\n",
            "iteration:2 step:87 loss:0.044262416660785675\n",
            "iteration:2 step:88 loss:0.011780044063925743\n",
            "iteration:2 step:89 loss:0.0127421198412776\n",
            "iteration:2 step:90 loss:0.04602499678730965\n",
            "iteration:2 step:91 loss:0.04982267692685127\n",
            "iteration:2 step:92 loss:0.043042223900556564\n",
            "iteration:2 step:93 loss:0.0655055120587349\n",
            "iteration:2 step:94 loss:0.008626115508377552\n",
            "iteration:2 step:95 loss:0.019160224124789238\n",
            "iteration:2 step:96 loss:0.02308080531656742\n",
            "iteration:2 step:97 loss:0.022780220955610275\n",
            "iteration:2 step:98 loss:0.07080874592065811\n",
            "iteration:2 step:99 loss:0.029480310156941414\n",
            "iteration:2 step:100 loss:0.022421542555093765\n",
            "iteration:2 step:101 loss:0.03772059082984924\n",
            "iteration:2 step:102 loss:0.015646230429410934\n",
            "iteration:2 step:103 loss:0.017044564709067345\n",
            "iteration:2 step:104 loss:0.030090952292084694\n",
            "iteration:2 step:105 loss:0.0224838238209486\n",
            "iteration:2 step:106 loss:0.02777329832315445\n",
            "iteration:2 step:107 loss:0.02636813558638096\n",
            "iteration:2 step:108 loss:0.01893499679863453\n",
            "iteration:2 step:109 loss:0.04514972120523453\n",
            "iteration:2 step:110 loss:0.022188499569892883\n",
            "iteration:2 step:111 loss:0.033505361527204514\n",
            "iteration:2 step:112 loss:0.02740994654595852\n",
            "iteration:2 step:113 loss:0.033419035375118256\n",
            "iteration:2 step:114 loss:0.024376681074500084\n",
            "iteration:2 step:115 loss:0.049420591443777084\n",
            "iteration:2 step:116 loss:0.013151481747627258\n",
            "iteration:2 step:117 loss:0.0198798980563879\n",
            "iteration:2 step:118 loss:0.022317150607705116\n",
            "iteration:2 step:119 loss:0.0439772829413414\n",
            "iteration:2 step:120 loss:0.022681277245283127\n",
            "iteration:2 step:121 loss:0.027475455775856972\n",
            "iteration:2 step:122 loss:0.024224262684583664\n",
            "iteration:2 step:123 loss:0.101023368537426\n",
            "iteration:2 step:124 loss:0.04919861629605293\n",
            "iteration:2 step:125 loss:0.03940894454717636\n",
            "iteration:2 step:126 loss:0.021846257150173187\n",
            "iteration:2 step:127 loss:0.03167977184057236\n",
            "iteration:2 step:128 loss:0.04269273579120636\n",
            "iteration:2 step:129 loss:0.02288547344505787\n",
            "iteration:2 step:130 loss:0.050087377429008484\n",
            "iteration:2 step:131 loss:0.020869815722107887\n",
            "iteration:2 step:132 loss:0.03405923396348953\n",
            "iteration:2 step:133 loss:0.05380433797836304\n",
            "iteration:2 step:134 loss:0.04963825270533562\n",
            "iteration:2 step:135 loss:0.01939421147108078\n",
            "iteration:2 step:136 loss:0.02600432187318802\n",
            "iteration:2 step:137 loss:0.026648880913853645\n",
            "iteration:2 step:138 loss:0.077817402780056\n",
            "iteration:2 step:139 loss:0.0452110692858696\n",
            "iteration:2 step:140 loss:0.03196387737989426\n",
            "iteration:2 step:141 loss:0.0365009568631649\n",
            "iteration:2 step:142 loss:0.019985124468803406\n",
            "iteration:2 step:143 loss:0.03239746764302254\n",
            "iteration:2 step:144 loss:0.022243337705731392\n",
            "iteration:2 step:145 loss:0.021374765783548355\n",
            "iteration:2 step:146 loss:0.02305316925048828\n",
            "iteration:2 step:147 loss:0.007812046445906162\n",
            "iteration:2 step:148 loss:0.019856296479701996\n",
            "iteration:2 step:149 loss:0.06186211109161377\n",
            "iteration:2 step:150 loss:0.019692031666636467\n",
            "iteration:2 step:151 loss:0.06131884083151817\n",
            "iteration:2 step:152 loss:0.056230850517749786\n",
            "iteration:2 step:153 loss:0.02331271395087242\n",
            "iteration:2 step:154 loss:0.03224433958530426\n",
            "iteration:2 step:155 loss:0.04199119657278061\n",
            "iteration:2 step:156 loss:0.04881627485156059\n",
            "iteration:2 step:157 loss:0.03322167322039604\n",
            "iteration:2 step:158 loss:0.049040935933589935\n",
            "iteration:2 step:159 loss:0.007495224010199308\n",
            "iteration:2 step:160 loss:0.017670350149273872\n",
            "iteration:2 step:161 loss:0.07338584214448929\n",
            "iteration:2 step:162 loss:0.06652236729860306\n",
            "iteration:2 step:163 loss:0.034988146275281906\n",
            "iteration:2 step:164 loss:0.03167838603258133\n",
            "iteration:2 step:165 loss:0.022139959037303925\n",
            "iteration:2 step:166 loss:0.017206929624080658\n",
            "iteration:2 step:167 loss:0.021926840767264366\n",
            "iteration:2 step:168 loss:0.08866171538829803\n",
            "iteration:2 step:169 loss:0.01622425951063633\n",
            "iteration:2 step:170 loss:0.0053079030476510525\n",
            "iteration:2 step:171 loss:0.042676325887441635\n",
            "iteration:2 step:172 loss:0.01403209287673235\n",
            "iteration:2 step:173 loss:0.04684293642640114\n",
            "iteration:2 step:174 loss:0.028248554095625877\n",
            "iteration:2 step:175 loss:0.045125797390937805\n",
            "iteration:2 step:176 loss:0.06173652037978172\n",
            "iteration:2 step:177 loss:0.024878045544028282\n",
            "iteration:2 step:178 loss:0.011102692224085331\n",
            "iteration:2 step:179 loss:0.010808303952217102\n",
            "iteration:2 step:180 loss:0.021180637180805206\n",
            "iteration:2 step:181 loss:0.03543112799525261\n",
            "iteration:2 step:182 loss:0.05013100802898407\n",
            "iteration:2 step:183 loss:0.013245897367596626\n",
            "iteration:2 step:184 loss:0.05754401162266731\n",
            "iteration:2 step:185 loss:0.03475816175341606\n",
            "iteration:2 step:186 loss:0.05512499064207077\n",
            "iteration:2 step:187 loss:0.0568757988512516\n",
            "iteration:2 step:188 loss:0.022507891058921814\n",
            "iteration:2 step:189 loss:0.04572879150509834\n",
            "iteration:2 step:190 loss:0.0038869488053023815\n",
            "iteration:2 step:191 loss:0.04506320133805275\n",
            "iteration:2 step:192 loss:0.024168189615011215\n",
            "iteration:2 step:193 loss:0.025449829176068306\n",
            "iteration:2 step:194 loss:0.02643655426800251\n",
            "iteration:2 step:195 loss:0.0057413759641349316\n",
            "iteration:2 step:196 loss:0.06349220871925354\n",
            "iteration:2 step:197 loss:0.03964897245168686\n",
            "iteration:2 step:198 loss:0.0048774369060993195\n",
            "iteration:2 step:199 loss:0.09353289753198624\n",
            "iteration:2 step:200 loss:0.0301065631210804\n",
            "iteration:2 step:201 loss:0.05069373548030853\n",
            "iteration:2 step:202 loss:0.01963253691792488\n",
            "iteration:2 step:203 loss:0.017308957874774933\n",
            "iteration:2 step:204 loss:0.01811678521335125\n",
            "iteration:2 step:205 loss:0.025853455066680908\n",
            "iteration:2 step:206 loss:0.06725765764713287\n",
            "iteration:2 step:207 loss:0.11524257063865662\n",
            "iteration:2 step:208 loss:0.10260171443223953\n",
            "iteration:2 step:209 loss:0.028874685987830162\n",
            "iteration:2 step:210 loss:0.07602253556251526\n",
            "iteration:2 step:211 loss:0.016065647825598717\n",
            "iteration:2 step:212 loss:0.09440769255161285\n",
            "iteration:2 step:213 loss:0.010147564113140106\n",
            "iteration:2 step:214 loss:0.03379804641008377\n",
            "iteration:2 step:215 loss:0.03757089748978615\n",
            "iteration:2 step:216 loss:0.031444475054740906\n",
            "iteration:2 step:217 loss:0.08279585093259811\n",
            "iteration:2 step:218 loss:0.011066549457609653\n",
            "iteration:2 step:219 loss:0.015440539456903934\n",
            "iteration:2 step:220 loss:0.030396774411201477\n",
            "iteration:2 step:221 loss:0.06425358355045319\n",
            "iteration:2 step:222 loss:0.018138613551855087\n",
            "iteration:2 step:223 loss:0.03562537208199501\n",
            "iteration:2 step:224 loss:0.03980385884642601\n",
            "iteration:2 step:225 loss:0.031177012249827385\n",
            "iteration:2 step:226 loss:0.039497267454862595\n",
            "iteration:2 step:227 loss:0.023485353216528893\n",
            "iteration:2 step:228 loss:0.048355262726545334\n",
            "iteration:2 step:229 loss:0.03453080356121063\n",
            "iteration:2 step:230 loss:0.026763517409563065\n",
            "iteration:2 step:231 loss:0.03002285212278366\n",
            "iteration:2 step:232 loss:0.03675174340605736\n",
            "iteration:2 step:233 loss:0.03818828985095024\n",
            "iteration:2 step:234 loss:0.03842126950621605\n",
            "iteration:2 step:235 loss:0.030758485198020935\n",
            "iteration:2 step:236 loss:0.01117664948105812\n",
            "iteration:2 step:237 loss:0.029589340090751648\n",
            "iteration:2 step:238 loss:0.026327062398195267\n",
            "iteration:2 step:239 loss:0.031065719202160835\n",
            "iteration:2 step:240 loss:0.026264125481247902\n",
            "iteration:2 step:241 loss:0.03704073280096054\n",
            "iteration:2 step:242 loss:0.021515697240829468\n",
            "iteration:2 step:243 loss:0.08852648735046387\n",
            "iteration:2 step:244 loss:0.025453276932239532\n",
            "iteration:2 step:245 loss:0.02578677423298359\n",
            "iteration:2 step:246 loss:0.05394914373755455\n",
            "iteration:2 step:247 loss:0.038540925830602646\n",
            "iteration:2 step:248 loss:0.05521393567323685\n",
            "iteration:2 step:249 loss:0.028844133019447327\n",
            "iteration:2 step:250 loss:0.044681474566459656\n",
            "iteration:2 step:251 loss:0.03303846716880798\n",
            "iteration:2 step:252 loss:0.0456903874874115\n",
            "iteration:2 step:253 loss:0.05916885286569595\n",
            "iteration:2 step:254 loss:0.018507083877921104\n",
            "iteration:2 step:255 loss:0.06012464687228203\n",
            "iteration:2 step:256 loss:0.02567444182932377\n",
            "iteration:2 step:257 loss:0.01008852943778038\n",
            "iteration:2 step:258 loss:0.014193068258464336\n",
            "iteration:2 step:259 loss:0.022277241572737694\n",
            "iteration:2 step:260 loss:0.05546880140900612\n",
            "iteration:2 step:261 loss:0.05500350892543793\n",
            "iteration:2 step:262 loss:0.02352844551205635\n",
            "iteration:2 step:263 loss:0.02299482375383377\n",
            "iteration:2 step:264 loss:0.009167942218482494\n",
            "iteration:2 step:265 loss:0.01823103241622448\n",
            "iteration:2 step:266 loss:0.045604415237903595\n",
            "iteration:2 step:267 loss:0.00706271780654788\n",
            "iteration:2 step:268 loss:0.09198307991027832\n",
            "iteration:2 step:269 loss:0.02302391454577446\n",
            "iteration:2 step:270 loss:0.04755614325404167\n",
            "iteration:2 step:271 loss:0.07761848717927933\n",
            "iteration:2 step:272 loss:0.04725222289562225\n",
            "iteration:2 step:273 loss:0.036874815821647644\n",
            "iteration:2 step:274 loss:0.021665796637535095\n",
            "iteration:2 step:275 loss:0.05121450498700142\n",
            "iteration:2 step:276 loss:0.02061055414378643\n",
            "iteration:2 step:277 loss:0.06370485574007034\n",
            "iteration:2 step:278 loss:0.06727574020624161\n",
            "iteration:2 step:279 loss:0.007975402288138866\n",
            "iteration:2 step:280 loss:0.025001609697937965\n",
            "iteration:2 step:281 loss:0.04518941789865494\n",
            "iteration:2 step:282 loss:0.08944952487945557\n",
            "iteration:2 step:283 loss:0.010040760971605778\n",
            "iteration:2 step:284 loss:0.06198335066437721\n",
            "iteration:2 step:285 loss:0.006876753177493811\n",
            "iteration:2 step:286 loss:0.019155558198690414\n",
            "iteration:2 step:287 loss:0.038362082093954086\n",
            "iteration:2 step:288 loss:0.027977580204606056\n",
            "iteration:2 step:289 loss:0.08246167749166489\n",
            "iteration:2 step:290 loss:0.017936674878001213\n",
            "iteration:2 step:291 loss:0.04043054208159447\n",
            "iteration:2 step:292 loss:0.08378937840461731\n",
            "iteration:2 step:293 loss:0.013065840117633343\n",
            "iteration:2 step:294 loss:0.022597741335630417\n",
            "iteration:2 step:295 loss:0.09238672256469727\n",
            "iteration:2 step:296 loss:0.013625736348330975\n",
            "iteration:2 step:297 loss:0.015925947576761246\n",
            "iteration:2 step:298 loss:0.021127523854374886\n",
            "iteration:2 step:299 loss:0.06028549373149872\n",
            "iteration:2 step:300 loss:0.019345013424754143\n",
            "iteration:2 step:301 loss:0.05733635649085045\n",
            "iteration:2 step:302 loss:0.04296781122684479\n",
            "iteration:2 step:303 loss:0.008926243521273136\n",
            "iteration:2 step:304 loss:0.048863485455513\n",
            "iteration:2 step:305 loss:0.009833773598074913\n",
            "iteration:2 step:306 loss:0.04114018753170967\n",
            "iteration:2 step:307 loss:0.1203407496213913\n",
            "iteration:2 step:308 loss:0.02365799993276596\n",
            "iteration:2 step:309 loss:0.020672962069511414\n",
            "iteration:2 step:310 loss:0.03322669491171837\n",
            "iteration:2 step:311 loss:0.03267377242445946\n",
            "iteration:2 step:312 loss:0.024553773924708366\n",
            "iteration:2 step:313 loss:0.025795822963118553\n",
            "iteration:2 step:314 loss:0.03137461841106415\n",
            "iteration:2 step:315 loss:0.019132176414132118\n",
            "iteration:2 step:316 loss:0.009833790361881256\n",
            "iteration:2 step:317 loss:0.02204141393303871\n",
            "iteration:2 step:318 loss:0.046918194741010666\n",
            "iteration:2 step:319 loss:0.027427788823843002\n",
            "iteration:2 step:320 loss:0.04838781803846359\n",
            "iteration:2 step:321 loss:0.03566324710845947\n",
            "iteration:2 step:322 loss:0.06853069365024567\n",
            "iteration:2 step:323 loss:0.0315367616713047\n",
            "iteration:2 step:324 loss:0.06696972995996475\n",
            "iteration:2 step:325 loss:0.01879086345434189\n",
            "iteration:2 step:326 loss:0.024167679250240326\n",
            "iteration:2 step:327 loss:0.051312774419784546\n",
            "iteration:2 step:328 loss:0.01649582013487816\n",
            "iteration:2 step:329 loss:0.02484768070280552\n",
            "iteration:2 step:330 loss:0.04865444824099541\n",
            "iteration:2 step:331 loss:0.025896240025758743\n",
            "iteration:2 step:332 loss:0.06728000938892365\n",
            "iteration:2 step:333 loss:0.016006838530302048\n",
            "iteration:2 step:334 loss:0.040380142629146576\n",
            "iteration:2 step:335 loss:0.05106964334845543\n",
            "iteration:2 step:336 loss:0.09547396749258041\n",
            "iteration:2 step:337 loss:0.016000086441636086\n",
            "iteration:2 step:338 loss:0.008490553125739098\n",
            "iteration:2 step:339 loss:0.027633102610707283\n",
            "iteration:2 step:340 loss:0.029854126274585724\n",
            "iteration:2 step:341 loss:0.05306755006313324\n",
            "iteration:2 step:342 loss:0.043403320014476776\n",
            "iteration:2 step:343 loss:0.025620074942708015\n",
            "iteration:2 step:344 loss:0.048627112060785294\n",
            "iteration:2 step:345 loss:0.012253391556441784\n",
            "iteration:2 step:346 loss:0.05598032847046852\n",
            "iteration:2 step:347 loss:0.0726819783449173\n",
            "iteration:2 step:348 loss:0.03908566012978554\n",
            "iteration:2 step:349 loss:0.016153641045093536\n",
            "iteration:2 step:350 loss:0.029120875522494316\n",
            "iteration:2 step:351 loss:0.020675480365753174\n",
            "iteration:2 step:352 loss:0.043964676558971405\n",
            "iteration:2 step:353 loss:0.04425298795104027\n",
            "iteration:2 step:354 loss:0.033449627459049225\n",
            "iteration:2 step:355 loss:0.051079150289297104\n",
            "iteration:2 step:356 loss:0.05048726126551628\n",
            "iteration:2 step:357 loss:0.017255185171961784\n",
            "iteration:2 step:358 loss:0.041880976408720016\n",
            "iteration:2 step:359 loss:0.05108977109193802\n",
            "iteration:2 step:360 loss:0.031551726162433624\n",
            "iteration:2 step:361 loss:0.07430833578109741\n",
            "iteration:2 step:362 loss:0.030920881778001785\n",
            "iteration:2 step:363 loss:0.010203372687101364\n",
            "iteration:2 step:364 loss:0.019800206646323204\n",
            "iteration:2 step:365 loss:0.06653233617544174\n",
            "iteration:2 step:366 loss:0.029379235580563545\n",
            "iteration:2 step:367 loss:0.06389043480157852\n",
            "iteration:2 step:368 loss:0.021494759246706963\n",
            "iteration:2 step:369 loss:0.08312949538230896\n",
            "iteration:2 step:370 loss:0.0669984519481659\n",
            "iteration:2 step:371 loss:0.08405682444572449\n",
            "iteration:2 step:372 loss:0.027501707896590233\n",
            "iteration:2 step:373 loss:0.013376927003264427\n",
            "iteration:2 step:374 loss:0.05627645552158356\n",
            "iteration:2 step:375 loss:0.02781638316810131\n",
            "iteration:2 step:376 loss:0.00709049217402935\n",
            "iteration:2 step:377 loss:0.027060331776738167\n",
            "iteration:2 step:378 loss:0.018723025918006897\n",
            "iteration:2 step:379 loss:0.03039136901497841\n",
            "iteration:2 step:380 loss:0.014599849469959736\n",
            "iteration:2 step:381 loss:0.025285854935646057\n",
            "iteration:2 step:382 loss:0.11308635771274567\n",
            "iteration:2 step:383 loss:0.033850327134132385\n",
            "iteration:2 step:384 loss:0.040119513869285583\n",
            "iteration:2 step:385 loss:0.016962960362434387\n",
            "iteration:2 step:386 loss:0.04745221883058548\n",
            "iteration:2 step:387 loss:0.06936025619506836\n",
            "iteration:2 step:388 loss:0.0151629988104105\n",
            "iteration:2 step:389 loss:0.06554025411605835\n",
            "iteration:2 step:390 loss:0.020939184352755547\n",
            "iteration:2 step:391 loss:0.012308917939662933\n",
            "iteration:2 step:392 loss:0.05316770076751709\n",
            "iteration:2 step:393 loss:0.0689781978726387\n",
            "iteration:2 step:394 loss:0.03539281710982323\n",
            "iteration:2 step:395 loss:0.019486313685774803\n",
            "iteration:2 step:396 loss:0.02652490884065628\n",
            "iteration:2 step:397 loss:0.03973154351115227\n",
            "iteration:2 step:398 loss:0.013964551500976086\n",
            "iteration:2 step:399 loss:0.0313449390232563\n",
            "iteration:2 step:400 loss:0.0656047835946083\n",
            "iteration:2 step:401 loss:0.03288380429148674\n",
            "iteration:2 step:402 loss:0.02073589339852333\n",
            "iteration:2 step:403 loss:0.01588493585586548\n",
            "iteration:2 step:404 loss:0.027372224256396294\n",
            "iteration:2 step:405 loss:0.01760748028755188\n",
            "iteration:2 step:406 loss:0.03199390694499016\n",
            "iteration:2 step:407 loss:0.03811642527580261\n",
            "iteration:2 step:408 loss:0.03669486194849014\n",
            "iteration:2 step:409 loss:0.03038635291159153\n",
            "iteration:2 step:410 loss:0.012033172883093357\n",
            "iteration:2 step:411 loss:0.02699608914554119\n",
            "iteration:2 step:412 loss:0.031303297728300095\n",
            "iteration:2 step:413 loss:0.05064450576901436\n",
            "iteration:2 step:414 loss:0.031829942017793655\n",
            "iteration:2 step:415 loss:0.023268166929483414\n",
            "iteration:2 step:416 loss:0.0049287606962025166\n",
            "iteration:2 step:417 loss:0.03799750655889511\n",
            "iteration:2 step:418 loss:0.018517576158046722\n",
            "iteration:2 step:419 loss:0.05635444447398186\n",
            "iteration:2 step:420 loss:0.011836838908493519\n",
            "iteration:2 step:421 loss:0.022029925137758255\n",
            "iteration:2 step:422 loss:0.035227544605731964\n",
            "iteration:2 step:423 loss:0.027964379638433456\n",
            "iteration:2 step:424 loss:0.010209169238805771\n",
            "iteration:2 step:425 loss:0.02193494327366352\n",
            "iteration:2 step:426 loss:0.03354042395949364\n",
            "iteration:2 step:427 loss:0.005669762380421162\n",
            "iteration:2 step:428 loss:0.04866563528776169\n",
            "iteration:2 step:429 loss:0.025210365653038025\n",
            "iteration:2 step:430 loss:0.00921498890966177\n",
            "iteration:2 step:431 loss:0.028705529868602753\n",
            "iteration:2 step:432 loss:0.061366692185401917\n",
            "iteration:2 step:433 loss:0.01996290683746338\n",
            "iteration:2 step:434 loss:0.02160162292420864\n",
            "iteration:2 step:435 loss:0.0408179871737957\n",
            "iteration:2 step:436 loss:0.02422046847641468\n",
            "iteration:2 step:437 loss:0.01502945739775896\n",
            "iteration:2 step:438 loss:0.02203013002872467\n",
            "iteration:2 step:439 loss:0.02737225592136383\n",
            "iteration:2 step:440 loss:0.0185024905949831\n",
            "iteration:2 step:441 loss:0.026819096878170967\n",
            "iteration:2 step:442 loss:0.03387323394417763\n",
            "iteration:2 step:443 loss:0.02707141637802124\n",
            "iteration:2 step:444 loss:0.010571879334747791\n",
            "iteration:2 step:445 loss:0.020569929853081703\n",
            "iteration:2 step:446 loss:0.010916614904999733\n",
            "iteration:2 step:447 loss:0.031954575330019\n",
            "iteration:2 step:448 loss:0.02362165041267872\n",
            "iteration:2 step:449 loss:0.030491089448332787\n",
            "iteration:2 step:450 loss:0.019978199154138565\n",
            "iteration:2 step:451 loss:0.04603174328804016\n",
            "iteration:2 step:452 loss:0.017952309921383858\n",
            "iteration:2 step:453 loss:0.04889971390366554\n",
            "iteration:2 step:454 loss:0.013777841813862324\n",
            "iteration:2 step:455 loss:0.005980597343295813\n",
            "iteration:2 step:456 loss:0.01390695758163929\n",
            "iteration:2 step:457 loss:0.00605431292206049\n",
            "iteration:2 step:458 loss:0.011101596988737583\n",
            "iteration:2 step:459 loss:0.023736173287034035\n",
            "iteration:2 step:460 loss:0.0020839227363467216\n",
            "iteration:2 step:461 loss:0.0033805908169597387\n",
            "iteration:2 step:462 loss:0.002831440418958664\n",
            "iteration:2 step:463 loss:0.03204210847616196\n",
            "iteration:2 step:464 loss:0.01977173425257206\n",
            "iteration:2 step:465 loss:0.006329372525215149\n",
            "iteration:2 step:466 loss:0.10590315610170364\n",
            "iteration:2 step:467 loss:0.00574256619438529\n",
            "iteration:2 step:468 loss:0.08531708270311356\n"
          ]
        }
      ],
      "source": [
        "number_of_steps=len(train_loader)\n",
        "for epochs in range(num_epochs):\n",
        "  for i,(images,labels) in enumerate(train_loader):\n",
        "    images=images.reshape(-1,28*28).to(device)\n",
        "    labels=labels.to(device)\n",
        "    #Forward_pass\n",
        "    outputs=model(images)\n",
        "    loss=criterion(outputs,labels)\n",
        "    #Backward_pass and updation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('iteration:'+str(epochs)+' step:'+str(i)+' loss:'+str(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVPf0SWz7H9M",
        "outputId": "7f56e060-ff65-4b7c-9036-945fca167298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96.54865506329114\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  n_samples=0\n",
        "  n_correct=0\n",
        "  for images,labels in test_loader:\n",
        "    images=images.reshape(-1,28*28).to(device)\n",
        "    labels=labels.to(device)\n",
        "    outputs=model(images)\n",
        "    _,predicted=torch.max(outputs.data,1)\n",
        "    n_correct+=(predicted==labels).sum().item()\n",
        "    n_samples+=label.size(0)\n",
        "accuracy=(n_correct*100)/n_samples\n",
        "print(accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
